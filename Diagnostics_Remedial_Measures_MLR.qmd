# Diagnostics and Remedial measures (MLR)

Let us consider the *Advertisement.csv* dataset. We will develop a multiple linear regression model to predict `sales` based on expenditure on advertisement via `TV` and `radio`.

## Loading libraries

```{r}
#| message: false
library(ggplot2) #for making plots
library(lmtest) #for bptest()
library(MASS) #for boxcox()
#library(rgl) #for 3D plots
```


## Reading data

```{r}
sales_data <- read.csv('./Datasets/Advertising.csv')
```

## Developing model

```{r}
linear_model <- lm(sales~TV+radio, data = sales_data)
```

## Model fit

With two predictors, the model fit can be visualized via a 3-dimensional plot *(2 predictors on 2 axes, and the response on the 3rd axis)*. However, with more than 2 predictors it will be impossible to visualize the model fit based on all the predictors and response. Thus, visualizing the response with the predictors is not an effective strategy. We will visualize the model fit via a 3D plot for this example since we have only 2 predictors, but that will be just for seeing the effectiveness of our approach of performing diagnostic tests and using remedial measure to develop a MLR model. We will not use the visualization of the response with the predictors to propose a model development strategy as such a strategy will fail for more than 2 predictors.

Let us check the goodness-of-fit of the model.

```{r}
summary(linear_model)
```

The model $R^2_{adj}$ is 90\% indicating a good-fit of the model to the data.


```{r}
#| echo: false
#| eval: false
plot3d( 
  x=sales_data$TV, y=sales_data$radio, z=sales_data$sales, 
  #col = data$color, 
  #type = 's', 
  radius = .1,
  xlab="TV", ylab="Radio", zlab="Sales")
plot3d(x=sales_data$TV, y=sales_data$radio, z=linear_model$fitted.values, colvar = linear_model$fitted.values, add = TRUE, col = 'blue')
```


## Diagnostic plots & statistical tests

Let us make diagnostic plots that help us check the model assumptions.

```{r}
par(mfrow = c(2,2))
plot(linear_model)
```

From the plot residuals vs fitted values, we see that the relationship does not seem to be linear. However, the homoscedasticity assumption seems to be only mildly violated.

From the Q-Q plot, we can see that the error distribution is left-skewed, and thus the assumption of normal distribution of errors is also violated.

Let us verify our visual check with statistical tests.

```{r}
bptest(linear_model)
shapiro.test(linear_model$residuals)
```

Considering a significance level of 5\%, the model does not satisfy the assumption of normal distribution of errors, but satisfies the homoscedasticity assumption.

Note that the statistical test for the linearity assumption is not possible since we don't have replicates for the combination of values of `TV` and `radio` expenditures as shown below.

```{r}
nrow(unique(sales_data[,c('TV', 'radio')]))
```

The data has 200 observations, and each of them is a unique combination of expenditure on `TV` and `radio`. Thus, we'll only use the diagnostic plots to check the linearity assumption.

In multiple linear regression, we also develop the residual plots against each predictor. These plots may indicate the predictor(s) that may not have linear relationship with the response.

```{r}
# Residuals vs TV expenditure
ggplot(data = sales_data, aes(x = TV, y = linear_model$residuals))+
  geom_point()+
  geom_smooth()
```

The relationship between `TV` expenditure and `sales` seems to be slightly non-linear. The constant error variance assumption is also violated.

```{r}
# Residuals vs radio expenditure
ggplot(data = sales_data, aes(x = radio, y = linear_model$residuals))+
  geom_point()+
  geom_smooth()
```

The relationship between `radio` expenditure and `sales` seems to be linear. However, the constant error variance assumption is  violated.


## Remedial measures

Based on the diagnostic plots, the linearity assumption and the assumptions regarding error distribution are violated. Since both linearity and error distribution assumptions are violated, we will recommend transforming the response `sales` instead of the predictors. Transforming `sales` is likely to change the nature of relationship between the response and the predictors, as well as the error distribution.

Let us use the Box-Cox procedure to identify the appropriate transformation of the response `sales`.

```{r}
par(mfrow=c(1,1))
boxcox(linear_model)
```

From the above plot, we observe that the Box-Cox procedure suggests no transformation of the response. However, as the residual distribution is left-skewed, a convex transformation of the response was expected. 

Note that the Box-Cox procedure does not guarantee the appropriate value of the power transformation $\lambda$ to transform the residual distribution to normal. It estimates the value of $\lamdba$ by maximizing the likelihood of the data based on the assumption of normal distribution of errors. Thus, it is likely to estimate the value of $\lambda$ that pushes the distribution of the residuals closer to the normal distribution. However, since we have finite data, there may exist other distributions that maximize the likelihood of the data by chance, resulting in an inappropriate value of $\lambda$.

As we can see that a convex transformation will be appropriate in this case, let us transform `sales` with one of the most popular and simple convex transformation, which is the polynomial transformation of degree 2.

```{r}
transformed_resp_model <- lm(sales^2 ~ TV+radio, data = sales_data)
summary(transformed_resp_model)
```

The transformed model with $R^2_{adj} = 86\%$ seems to have a slightly worse goodness-of-fit as compared to the previous model. Let us make diagnostic plots to check model assumptions.

```{r}
par(mfrow=c(2,2))
plot(transformed_resp_model)
```

We observe that there is an even stronger departure from the linear relationship assumption as compared to the previous model. However, the error distribution seems to be normal. There may be slight departure from the homoscedasticity assumption. Let us verify the homoscedasticity and normality assumptions with statistical tests.

```{r}
bptest(transformed_resp_model)
shapiro.test(transformed_resp_model$residuals)
```


