[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression analysis (R notes)",
    "section": "",
    "text": "Preface\nThese are coding notes for the course STAT350. If you don’t have a background in R, please use the course material on R from the STAT201 course here."
  },
  {
    "objectID": "Rintro.html#installing-r",
    "href": "Rintro.html#installing-r",
    "title": "1  R: Introduction",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nGo to the The Comprehensive R Archive Network (CRAN)\n\n\n\nCRAN\n\n\nUnder “Download and Install R,” choose “Linux,” “MacOS X” or “Windows.” If you choose Windows, on the next page choose “base,” and on the following page choose “Download R 4.3.1 for Windows” to download the setup program.\nIf you choose MacOS X or Linux you will need to read through the instructions to find the downloads you need for your machine.\nOnce you have downloaded the setup program, execute it and follow the instructions for installing R on your system. If you have an earlier version of R already installed, you may continue to use it, or you can uninstall it and then install the most recent version, which is R 4.3.1."
  },
  {
    "objectID": "Rintro.html#installing-rstudio",
    "href": "Rintro.html#installing-rstudio",
    "title": "1  R: Introduction",
    "section": "1.2 Installing RStudio",
    "text": "1.2 Installing RStudio\nhttps://rstudio.com/products/rstudio/download/\nChoose your version: RStudio Desktop, Open Source License, Free. It is strongly recommended that you use the latest release of RStudio (v2023.06). After you install RStudio, you can double click on it and open:\n\n\n\nR Studio\n\n\nUsually you will want to import data from a file corresponding to data associated with a homework problem. Such a file will usually end with the extensions *.txt or *.dat. The data files for this course will always be available on the CD that comes with the text and/or on the course web page. A data file will consist of columns of numbers, with nothing separating the columns but “white space.” If each column has a title on top describing what the data in the column represents (e.g., age, weight, income, etc.), we will say that the file has a header."
  },
  {
    "objectID": "Rintro.html#working-directory",
    "href": "Rintro.html#working-directory",
    "title": "1  R: Introduction",
    "section": "1.3 Working directory",
    "text": "1.3 Working directory\nThe easiest way to import the data into R and have it readily available for the current and future sessions is to first save the data file into your working directory. For example mine is C:\\stat350.\nTo set up the working directory, select the project option by choosing File menu, then New Project, and then Create Project from Existing Directory.\nTo start writing a new R script, navigate to the New File option in the File menu, and select Quarto Document. This will create a *.qmd file. You can write both code and formatted-text in this document. When working on assignment / exam problems, you will work on the *.qmd file, render it as HTML and then submit. You can view some examples on how to write R code and text in a *.qmd file and render it as HTML here.\nFor rough work, i.e., work that won’t be graded, you may use the R script option to write code."
  },
  {
    "objectID": "Rintro.html#getting-started-with-code",
    "href": "Rintro.html#getting-started-with-code",
    "title": "1  R: Introduction",
    "section": "1.4 Getting started with code",
    "text": "1.4 Getting started with code\n\n1.4.1 Reading data\nSuppose you want to work with the data from Problem 19 of Chapter 1, which is in a file named CH01PR19.txt which you have saved from the CD or the course web page in the Datasets folder within your R working directory. Assume the file has no header. You will want to create a Table object in R containing this data. First choose an appropriate name for the table. Assume you choose to name it Data. Then, you can execute the following code :\n\nData &lt;- read.table(\"./Datasets/CH01PR19.txt\")\n\nThen there will be a Table object in R named Data containing the data in rows and columns. To view it, you would type\n\nData\n\nHowever, if it is a large file, you might not be able to view the whole table at once. In that case, you may use the head() function, which will display only the first 6 rows of Data:\n\nhead(Data)\n\n     V1 V2\n1 3.897 21\n2 3.885 14\n3 3.778 28\n4 2.540 22\n5 3.028 21\n6 3.865 31\n\n\nNote that, in the absence of a header, the columns will be named V1, V2, etc., and the rows will be numbered.\nNow if the file does have a header (which you may have added yourself), you need to change the above command to:\n\nData &lt;- read.table(\"CH01PR19.txt\", header=TRUE)\n\nIn this case, when you view the file you will see the title for each column at the top of each column instead of V1, V2, etc. R regards these titles as names for the columns, and not as data.\nIf you want to load the data file from some other directory, you need to type the full path name in the read.table() command. For instance,\n\nData &lt;- read.table(file=\"C:/stat350/CH01PR19.txt\", header=FALSE) \n\nYou may read data manually as well. Here both Return and New are vectors.\n\nReturn &lt;- c(74,66,81,52,73,62,52,45,62,46,60,46,38)\nNew &lt;-c(5,6,8,11,12,15,16,17,18,18,19,20,20)\n\n\n\n1.4.2 Renaming columns\nNow suppose the file Data has two columns, and the first column is the GPA, while the second column is ACT score. If you would like to rename the columns in your R data table so that each column has a descriptive title, you could give the R command:\n\nnames(Data) &lt;- c(\"GPA\", \"ACT\")\n\nThen when you view the file the titles of the columns will have the new names you assigned:\n\nhead(Data)\n\n    GPA ACT\n1 3.897  21\n2 3.885  14\n3 3.778  28\n4 2.540  22\n5 3.028  21\n6 3.865  31\n\n\nNote that you can also give the columns these titles in the data file before you load it into R, and then use the header = TRUE setting when loading. Also, to avoid errors, you should never include a space in the title of any column\n\n\n1.4.3 Exporting data\nSuppose you wish to export Data to file Intro.csv in your folder.\n\nwrite.table(Data, \"C:/stat350/Intro.csv\", col.names=TRUE, sep=\",\")\n\nSuppose you wish to export Data to Intro.txt with a tab delimiter:\n\nwrite.table(Data, \"C:/stat350/Intro.txt\", col.names=TRUE, sep=\"\\t\")\n\nYou may export R objects to other file types in a similar manner.\n\n\n1.4.4 R environment\nIf you want to see which R objects are currently in your R environment, you can type:\n\nls()\n\nYou may also see these objects at the top right corner of the R Studio interface.\nIf you no longer need one or more of these objects, you can remove them. For instance, if you are done with Data, you can type:\n\nrm(Data)\n\nThen Data will no longer be in your current R environment. When you quit R, if you wish to keep all the new objects in your current R environment, be sure to answer Yes when asked, Save workspace image?\n\n\n1.4.5 Scatter plots and simple linear regression\nSuppose the data for Problem 19 of Chapter One has been stored in an R object named Data which has two columns, the first column named GPA and the second column named ACT. You want to make a scatterplot in R with ACT scores on the horizontal axis and GPA on the vertical axis. The R command is:\n\nplot(Data$ACT, Data$GPA)\n\n\n\n\nNote that the dollar sign is used to reference either column in the table named Data. The first argument to the plot() function is the column corresponding to the variable associated with the horizontal axis, and the second argument is the column corresponding to the variable associated with the vertical axis. Alternately, you could define two new vector variables, X and Y, to hold the data of the individual columns, and use these vectors as the arguments to the plot() function:\n\nX &lt;- Data$ACT\nY &lt;- Data$GPA\nplot(X, Y)\n\nFor now we will stick with the former approach. The resulting plot appears in the R Graphics Device within the R interface. Click on it to view it, save it, print it, etc.\nNote that whenever you make a new plot the old one will disappear (this can be changed; but not easily), so save it if you don’t want to lose it. However, the current scatterplot is inadequate. It has no title, the axis labels aren’t very informative, and the points are open circles rather than dark filled-in circles. To fix this, we can add some additional settings to the plot() command:\n\nplot(Data$ACT, Data$GPA, main=\"Problem 1.19\", xlab=\"ACT Test Score\", ylab=\"Freshman GPA\", pch=19)\n\n\n\n\nNow we obtain a much nicer scatterplot.\nWhatever you put in quotes after main = will be the title for the plot. Whatever you put in quotes after xlab = and ylab = will the the labels for the horizontal and vertical axes, respectively. The number after pch = is a code for the symbol to use for the points. You can try other numbers from 1 to 25. You can also use any symbol on your keyboard for the points, including numerals and letters, using quotes. For instance, if you want to use an asterisk for the points, type pch=\"*\".\nYou may want to also add a plot of the estimated regression function to the scatterplot of the data. This assumes you have already obtained the least squares estimates of the regression coefficients (see “Simple Linear Regression in R”).\n\nfit &lt;- lm(Data$GPA ~ Data$ACT)\nfit &lt;- lm(GPA~ACT, data=Data)   # another option\nplot(Data$ACT, Data$GPA, main=\"Problem 1.19\", xlab=\"ACT Test Score\", ylab=\"Freshman GPA\", pch=19)\nabline(fit, col = \"red\", lwd = 2) #lwd is for line-width\n\n\n\n\nThe line will appear superimposed over the data. You can also just type the actual values for the estimated intercept and slope if you prefer.\nYou may also use ggplot2 to make plots if you wish. ggplot() has a more intuitive syntax as it is based on the Grammar of Graphics, and also has more comprehensive formatting options.\n\nlibrary(ggplot2)\nggplot(Data, aes(x = ACT, y = GPA))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n  labs(\n    title = \"Problem 1.19\"\n  )\n\n$title\n[1] \"Problem 1.19\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nTo save your plot, click anywhere on the plot, then on the menu bar choose File, then Save as. Choose the format in which you want to save the plot, then where you want to save it on your drive.\nCheck the estimates for the intercept and slope:\n\nfit\n\n\nCall:\nlm(formula = GPA ~ ACT, data = Data)\n\nCoefficients:\n(Intercept)          ACT  \n    2.11405      0.03883  \n\n\nCompute fitted values:\n\nfit$fitted.values \n\nCompute residuals:\n\nfit$residuals\n\nCompute the estimate of \\(\\sigma^2\\), that is, the MSE:\n\nn &lt;- dim(Data)[1]\nsum(fit$residuals^2)/(n-2)\n\n[1] 0.3882848"
  },
  {
    "objectID": "Rintro.html#formatting-.qmd-file",
    "href": "Rintro.html#formatting-.qmd-file",
    "title": "1  R: Introduction",
    "section": "1.5 Formatting *.qmd file:",
    "text": "1.5 Formatting *.qmd file:\nBefore Quarto, *.Rmd files were commonly used to render HTML files with R code and formatted-text. This Cheatsheet is for formatting *.Rmd files. However, you may use it to format *.qmd files as well."
  },
  {
    "objectID": "Rintro.html#some-references-about-using-r",
    "href": "Rintro.html#some-references-about-using-r",
    "title": "1  R: Introduction",
    "section": "1.6 Some references about using R:",
    "text": "1.6 Some references about using R:\n\n100 page Introduction to R from the R website http://www.ics.uci.edu/~jutts/st108/R-intro.pdf\nPractical Regression and Anova using R, by Julian Faraway http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf\nR code by Bryan Goodrich for Kutner et al., Applied Linear Statistical Models 5th ed: https://rpubs.com/bryangoodrich"
  },
  {
    "objectID": "t_test_example.html#question",
    "href": "t_test_example.html#question",
    "title": "2  Hypothesis testing example",
    "section": "2.1 Question",
    "text": "2.1 Question\nCola manufacturers want to test how much the sweetness of a new cola drink is affected by storage. The sweetness loss due to storage was evaluated by 10 professional tasters (by comparing the sweetness before and after storage):\nTaster          Sweetness loss\n\n 1         2.0\n 2         0.4\n 3         0.7  \n 4         2.0  \n 5       −0.4   \n 6         2.2  \n 7       −1.3   \n 8         1.2  \n 9         1.1\n10         2.3\nObviously, we want to test if storage results in a loss of sweetness\nLet \\(\\mu\\) denote the sweetness loss, thus:\nNull hypothesis: \\(H_0: \\mu = 0\\)\nAlternate hypothesis: \\(H_a: \\mu &gt; 0\\)"
  },
  {
    "objectID": "t_test_example.html#solution",
    "href": "t_test_example.html#solution",
    "title": "2  Hypothesis testing example",
    "section": "2.2 Solution",
    "text": "2.2 Solution\nSample mean (\\(\\bar{x}\\)):\n\ndata &lt;- c(2, 0.4, 0.7, 2, -0.4, 2.2, -1.3, 1.2, 1.1, 2.3)\n\nxbar &lt;- mean(data)\nxbar\n\n[1] 1.02\n\n\nT-statistic:\n\nt = xbar/(sd(data)/sqrt(10))\nt\n\n[1] 2.696689\n\n\np-value:\n\n1-pt(t, df = 9)\n\n[1] 0.01226316\n\n\nIf the probability of Type I error considered is 5%, then we reject the null hypothesis, and conclude that the sweetness loss is indeed greater than 0.\nIf the probability of Type I error considered is 1%, then we fail to reject the null hypothesis, and conclude that the sweetness loss is indeed 0."
  },
  {
    "objectID": "SLR_model.html#read-data",
    "href": "SLR_model.html#read-data",
    "title": "3  Simple linear regression model: Example",
    "section": "3.1 Read data",
    "text": "3.1 Read data\n\nsenic &lt;- read.table('./Datasets/SENIC_data.txt')"
  },
  {
    "objectID": "SLR_model.html#data-pre-processing",
    "href": "SLR_model.html#data-pre-processing",
    "title": "3  Simple linear regression model: Example",
    "section": "3.2 Data pre-processing",
    "text": "3.2 Data pre-processing\n\ncolnames(senic) &lt;- c(\"ID\", \"LOS\", \"AGE\", \"INFRISK\", \"CULT\", \"XRAY\", \"BEDS\", \"MEDSCHL\", \"REGION\", \"CENSUS\", \"NURSE\", \"FACS\")"
  },
  {
    "objectID": "SLR_model.html#model",
    "href": "SLR_model.html#model",
    "title": "3  Simple linear regression model: Example",
    "section": "3.3 Model",
    "text": "3.3 Model\nDevelop a linear regression model to Predict the length of stay based on probability of the person getting infected.\n\nmodel &lt;- lm(LOS~INFRISK, data = senic)\nsummary(model)\n\n\nCall:\nlm(formula = LOS ~ INFRISK, data = senic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0587 -0.7776 -0.1487  0.7159  8.2805 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.3368     0.5213  12.156  &lt; 2e-16 ***\nINFRISK       0.7604     0.1144   6.645 1.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.624 on 111 degrees of freedom\nMultiple R-squared:  0.2846,    Adjusted R-squared:  0.2781 \nF-statistic: 44.15 on 1 and 111 DF,  p-value: 1.177e-09\n\n\n\nlibrary(ggplot2)\nggplot(data = senic, aes(x = INFRISK, y = LOS))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "SLR_model.html#error-variance",
    "href": "SLR_model.html#error-variance",
    "title": "3  Simple linear regression model: Example",
    "section": "3.4 Error variance",
    "text": "3.4 Error variance\n\nsum(model$residuals**2)/(111)\n\n[1] 2.637518"
  },
  {
    "objectID": "SLR_model.html#confidence-and-prediction-intervals",
    "href": "SLR_model.html#confidence-and-prediction-intervals",
    "title": "3  Simple linear regression model: Example",
    "section": "3.5 Confidence and Prediction intervals",
    "text": "3.5 Confidence and Prediction intervals\n\nlibrary(ggplot2)\n\nci &lt;- predict(model, newdata = senic, interval = \"confidence\", level = 0.95)\n\npi &lt;- predict(model, newdata = senic, interval = \"prediction\", level = 0.95)\n\ndata_ci &lt;- cbind(senic, ci)\ndata_pi &lt;- cbind(senic, pi)\n\nggplot(data = senic, aes(y = LOS, x = INFRISK))+\n  geom_point()+\n  geom_line(data = data_ci, aes(x = INFRISK, y = lwr), color = \"red\")+\n  geom_line(data = data_ci, aes(x = INFRISK, y = upr), color = \"red\")+\n  geom_line(data = data_pi, aes(x = INFRISK, y = lwr), color = \"green\")+\n  geom_line(data = data_pi, aes(x = INFRISK, y = upr), color = \"green\")+\n  geom_smooth(method = \"lm\")+\n  labs(\n    y = \"Length of stay\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#loading-libraries",
    "href": "Diagnostics_Remedial_Measures.html#loading-libraries",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.1 Loading libraries",
    "text": "4.1 Loading libraries\n\nlibrary(ISLR) #for the dataset \nlibrary(ggplot2) #for making plots\nlibrary(lmtest) #for bptest()\nlibrary(MASS) #for boxcox()"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#reading-data",
    "href": "Diagnostics_Remedial_Measures.html#reading-data",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.2 Reading data",
    "text": "4.2 Reading data\n\nauto_data &lt;- Auto"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#developing-model",
    "href": "Diagnostics_Remedial_Measures.html#developing-model",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.3 Developing model",
    "text": "4.3 Developing model\nSuppose we wish to predict mpg based on horsepower.\n\nlinear_model &lt;- lm(mpg~horsepower, data = auto_data)"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#model-fit",
    "href": "Diagnostics_Remedial_Measures.html#model-fit",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.4 Model fit",
    "text": "4.4 Model fit\nLet us visualize the model fit to the data.\n\nggplot(data = auto_data, aes(x = horsepower, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe plot indicates that a quadratic relationship between mpg and horsepower may be more appropriate."
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#diagnostic-plots-statistical-tests",
    "href": "Diagnostics_Remedial_Measures.html#diagnostic-plots-statistical-tests",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.5 Diagnostic plots & statistical tests",
    "text": "4.5 Diagnostic plots & statistical tests\nLet us make diagnostic plots that help us check the model assumptions.\n\npar(mfrow = c(2,2))\nplot(linear_model)\n\n\n\n\n\n4.5.1 Linear relationship\n\n4.5.1.1 Visual check\nWe can check the linearity assumption visually by analyzing the plot of residuals against fitted values. The plot indicates that a quadratic relationship between the response and the predictor may be more appropriate than a linear one.\n\n\n4.5.1.2 Statistical test\nWe’ll use the F test for lack of fit to check the linearity assumption\n\n#Full model\nfull_model &lt;- lm(mpg~as.factor(horsepower), data = auto_data)\n\n\n# F test for lack of fit\nanova(linear_model, full_model)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ horsepower\nModel 2: mpg ~ as.factor(horsepower)\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    390 9385.9                                 \n2    299 4702.8 91    4683.1 3.272 1.125e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs expected based on the visual check, the linear model fails the linearity test with a very low \\(p\\)-value of the order of \\(10^{-14}\\). Thus, we conclude that relationship is non-linear.\n\n\n\n4.5.2 Constant variance (Homoscedasticity)\n\n4.5.2.1 Visual check\nThe plot of the residuals against fitted values indicates that the error variance is increasing with increasing values of predicted mpg or decreasing horsepower. Thus, there seems to be heteroscedasticity.\n\n\n4.5.2.2 Statistical test\nBreusch-Pagan test\nLet us conduct the Breusch-Pagan test for homoscedasticity. This is a large sample test, which assumes that the error terms are independent and normally distruted, and that the variance of of the error term \\(\\epsilon_i\\), denoted by \\(\\sigma_i^2\\), is related to the level of the predictor \\(X\\) in the following way:\n\\(\\log(\\sigma_i^2) = \\gamma_0 + \\gamma_1X_i\\).\nThis implies that \\(\\sigma_i^2\\) either increases or decreases with the level of \\(X\\). Constant error variance corresponds to \\(\\gamma_1 = 0\\). The hypotheses for the test are:\n\\(H_0: \\gamma_1 = 0\\) \\(H_a: \\gamma_1 \\ne 0\\)\n\nbptest(linear_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  linear_model\nBP = 8.7535, df = 1, p-value = 0.00309\n\n\nAs expected based on the visual check, the model fails the homoscedasticity with a low \\(p\\)-value of 0.3%.\nBrown-Forsythe test\nTo conduct the Brown-Forsythe test, we divide the data into two groups, based on the predictor \\(X\\). If the error variance is not constant, the residuals in one group will tend to be more variable than those in the other group. The test consists of a two-sample \\(t\\)-test to determine whether the mean of the absolute deviations from the median residual of one group differs significantly from the mean of the absolute deviations from the median residual of the other group.\n\n#Break the residuals into 2 groups\nresiduals_group1 &lt;- linear_model$residuals[auto_data$horsepower&lt;=100]\nresiduals_group2 &lt;- linear_model$residuals[auto_data$horsepower&gt;100]\n\n\n#Obtain the median of each group\nmedian_group1 &lt;- median(residuals_group1)\nmedian_group2 &lt;- median(residuals_group2)\n\n\n#Two-sample t-test\nt.test(abs(residuals_group1-median_group1), abs(residuals_group2-median_group2), var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  abs(residuals_group1 - median_group1) and abs(residuals_group2 - median_group2)\nt = 4.4039, df = 390, p-value = 1.375e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.7738625 2.0220633\nsample estimates:\nmean of x mean of y \n 4.308698  2.910735 \n\n\nBrown-Forsythe test also shows that there is heteroscedasticity.\n\n\n\n4.5.3 Normality of error terms\n\n4.5.3.1 Visual check\nThe QQ plot indicates that the distribution of the residuals is slightly right-skewed. This right-skew can be visualized by making a histogram or a density plot of residuals:\n\npar(mfrow = c(1,1))\nhist(linear_model$residuals, breaks = 20, prob = TRUE)\nlines(density(linear_model$residuals), col = 'blue')\n\n\n\n\n\n\n4.5.3.2 Statistical test\n\nshapiro.test(linear_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  linear_model$residuals\nW = 0.98207, p-value = 8.734e-05\n\n\nAs expected based on the visual check, the model fails the test for normal distribution of error terms with a low \\(p\\)-value of the order of \\(10^{-5}\\).\n\n\n\n4.5.4 Independence of error terms\nThis test is relevant if there is an order in the data, i.e., if the observations can be arranged based on time, space, etc. In the current example, there is no order, and so this test is not relevant."
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#remedial-measures",
    "href": "Diagnostics_Remedial_Measures.html#remedial-measures",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.6 Remedial Measures",
    "text": "4.6 Remedial Measures\nIf the linear relationship assumption violated, it results in biased estimates of the regression coefficients resulting in a biased prediction. As the estimates are biased, their variance estimates may also be inaccurate. Thus, this is a very important assumption as it leads to inaccuracies in both point estimates and statistical inference.\nIf the homoscedasticity assumption is violated, but the linear relationship assumption holds, the OLS estimates are still unbiased resulting in an unbiased prediction. However, their standard error estimates are likely to be inaccurate.\nIf the assumption regarding the normal distribution of errors is violated, the OLS estimates are still BLUE (Best linear unbiased estimates). The confidence interval for a point estimate, though effected, is robust to departures from normality for large sample sizes. However, the prediction interval is sensitive to the same.\nOur model above (linear_model) violates all the three assumptions.\nNote that there is no one unique way of fixing all the model assumption violations. Its an iterative process, and there is some trial and error involved. Two different approaches may lead to two different models, and both models may be more or less similar (in terms of point estimates / inference)\n\n4.6.1 Addressing linear relationship assumption violation\nLet us address the linear relationship assumption first.\nAs indicated in the diagnostic plots, the relationship between \\(X\\) and \\(Y\\) seemed to be quadratic. We’ll change the simple linear regression model to the following linear regression model, where \\(Y\\) and \\(X\\) have a quadratic relationship:\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2\\)\nLet us fit the above model to the data:\n\nquadratic_model &lt;- lm(mpg~horsepower + I(horsepower**2), data = auto_data)\n\nNote that in the formula specified within the lm() function, the I() operator isolates or insulates the contents within I(…) from the regular formula operators. Without the I() operator, horsepower**2 will be treated as the interaction of horsepower with itself, which is horsepower. Thus, to add the square of horsepower as a separate predictor, we need to use the I() operator.\nAfter every transformation or remedial measure, we should diagnoze the model to check the model assumptions are satisfied.\n\npar(mfrow=c(2,2))\nplot(quadratic_model)\n\n\n\n\nWe observe that the departure from the linear relationship assumption has been reduced. However, there still seems to be heteroscedasticity and non-normal error terms. Let us verify this with statistical tests.\n\n#F test for lack of fit\nanova(quadratic_model, full_model)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ horsepower + I(horsepower^2)\nModel 2: mpg ~ as.factor(horsepower)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    389 7442.0                                  \n2    299 4702.8 90    2739.3 1.9351 1.897e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEven though the model still fails the linearity test, the extent of violation is reduced as the \\(p\\)-value is now of the order of \\(10^{-5}\\) instead of \\(10^{-14}\\).\n\n# Breusch-Pagan test\nbptest(quadratic_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  quadratic_model\nBP = 34.528, df = 2, p-value = 3.179e-08\n\n\nThe quadratic model has a more severe heteroscedasticity as compared to the linear model based on the \\(p\\)-value.\n\nshapiro.test(quadratic_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  quadratic_model$residuals\nW = 0.98268, p-value = 0.0001214\n\n\nThe severity of departure from normality has been reduced in the quadratic model.\n\n\n4.6.2 Addressing heteroscedasticity\nLet us address the heteroscedasticity assumption.\nWe can see from the diagnostic plot of the quadratic model that the error variance is higher for higher fitted values. A possible way to resolve this is to transform the response \\(Y\\) using a concave function such as log(\\(Y\\)) or \\(\\sqrt{Y}\\). Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. Let us try the log(\\(Y\\)) transformation to address heteroscedasticity.\n\nlog_model &lt;- lm(log(mpg)~horsepower+I(horsepower**2), data = auto_data)\npar(mfrow=c(2,2))\nplot(log_model)\n\n\n\n\nBased on the plot of residuals against fitted values, heteroscedasticity seems to be mitigated to some extent. Let us confirm with the statistical test.\n\nbptest(log_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  log_model\nBP = 5.2023, df = 2, p-value = 0.07419\n\n\nAssuming a significance level of \\(5\\%\\), we do not reject the null hypothesis that the error variance is constant. Thus, the homoscedasticity assumption holds on the log transformed model.\nThe linear relationship assumptions seems to be satisfied. Let us check it via the \\(F\\) lack of fit test.\n\nlog_full_model &lt;- lm(log(mpg)~as.factor(horsepower), data = auto_data)\nanova(log_model, log_full_model)\n\nAnalysis of Variance Table\n\nModel 1: log(mpg) ~ horsepower + I(horsepower^2)\nModel 2: log(mpg) ~ as.factor(horsepower)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    389 12.100                                  \n2    299  7.586 90    4.5138 1.9768 1.032e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlthough the \\(p\\)-value is higher than both the previous models indicating that the departure from linear relationship has been reduced, the model fails the \\(F\\) lack of fit test. However, from the residual plot, there didn’t seem to be a strong departure from the linear relationship assumption. The failure in the statistical test may be due to the presence of outliers in the data. Let us visualize the model full model along with the\n\ncolors &lt;- c(\"Full log model\" = \"red\", \"Log linear model\" = \"blue\")\n\nggplot(data = auto_data, aes(x = horsepower, y = log(mpg)))+\n  geom_point(col = \"orange\")+\n  geom_line(aes(y = log_full_model$fitted.values, color = \"Full log model\"))+\n  geom_line(aes(y = log_model$fitted.values, color = \"Log linear model\"))+\n    scale_color_manual(values = colors)+\n  theme(legend.title = element_blank(),\n        legend.position = c(0.85,0.9))\n\n\n\n\nWe observe that the full log model seems to be effected by outlying observations and explaining some of the noise. With the addition of more replicates at the \\(X\\) values corresponding to the outlying observations, the model may have passed the \\(F\\) lack of fit test. Thus, in this case, it appears that the relationship is indeed linear, but the failure in the statistical test is due to the full model over-fitting the data and explaining some noise as well. Another option is to bin the horsepower variable such that each bin has several replicates, and then do the \\(F\\) lack of fit test.\nFrom the QQ plot, it appears that the error distribution is normal, except for three outlying values. Let us confirm the same from the normality test.\n\nshapiro.test(log_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_model$residuals\nW = 0.99154, p-value = 0.02458\n\n\nThe \\(p\\)-value is high supporting the claim that the model satisfies the assumption of normality of error terms. The \\(p\\)-value would probably be higher if it was not for those three outlying points. The next step could be to analyze those three points to figure out the reason they are outlying values. For example, those three points may correspond to a particular car model that has only three observations in the data."
  },
  {
    "objectID": "Diagnostics_Remedial_Measures.html#box-cox-transformation",
    "href": "Diagnostics_Remedial_Measures.html#box-cox-transformation",
    "title": "4  Diagnostics and Remedial measures (SLR)",
    "section": "4.7 Box-Cox transformation",
    "text": "4.7 Box-Cox transformation\nSometimes it may be difficult to determine the appropriate transformation from diagnostic plots to correct the skewness of the error terms, unequal error variances, and non-linearity of the error function. The Box-Cox procedure automatically identifies a transformation from the family of power transformations on \\(Y\\). The family of power transformations is of the form:\n\\(Y' = Y^{\\lambda}\\)\nThe normal error regression model with the response variable a member of the family of power transformations becomes:\n\\(Y_i^{\\lambda} = \\beta_0 + \\beta_1X_i\\)\nThe Box-Cox procedure uses the method of Maximum likelihood to estimate \\(\\lambda\\), as well as other parameters \\(\\beta_0, \\beta_1\\), and \\(\\sigma^2\\).\nLet us use the Box-Cox procedure to identify the appropriate transformation for the linear_model.\n\nboxcox(linear_model)\n\n\n\n\nA plot of the log-likelihood vs \\(\\lambda\\) is returned with a confidence interval for the optimal value of \\(\\lambda\\). We can zoom-in the plot to identify the appropriate value of \\(\\lambda\\) where the log-likelihood maximizes.\n\nboxcox(linear_model, lambda = seq(-1, 0, 0.1))\n\n\n\n\nA value of \\(\\lambda = -0.5\\) seems appropriate. Note that the log-likelihood is based on a sample. Thus, typically we don’t select the exact value of \\(\\lambda\\) where the log-likeliood maximizes, but any value that is easier to interpret within the confidence interval. Let us consider \\(\\lambda = -0.5\\) in this case.\n\nboxcox_model &lt;- lm(mpg^-0.5~horsepower, data = auto_data)\npar(mfrow=c(2,2))\nplot(boxcox_model)\n\n\n\n\nBased on the plots, the model seems to satisfy the normality of errors and homoscedasticity assumption, but there appears to be a quadratic relationship between the response and the predictor. Let us confirm our visual analysis with statistical tests.\n\nbptest(boxcox_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  boxcox_model\nBP = 10.484, df = 1, p-value = 0.001204\n\n\nThe homoscedasticity assumption is violated, but not too severely.\n\nshapiro.test(boxcox_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  boxcox_model$residuals\nW = 0.99449, p-value = 0.1721\n\n\nBox-Cox transformation indeed resulted in the normal distribution of error terms.\n\npower_full_model &lt;- lm(mpg^-0.5~as.factor(horsepower), data = auto_data)\nanova(boxcox_model, power_full_model)\n\nAnalysis of Variance Table\n\nModel 1: mpg^-0.5 ~ horsepower\nModel 2: mpg^-0.5 ~ as.factor(horsepower)\n  Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    390 0.152325                                  \n2    299 0.085073 91  0.067252 2.5974 6.074e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs expected based on the visual analysis, the relationship has a large departure from linearity.\nNote that the Box-Cox transformation can identify the appropriate transformation for the response, but not for the predictors. Thus, we need to figure out the appropriate predictor transformation based on the diagnostic plots and/or trial and error. Let us try the quadratic transformation of the predictor \\(X\\) as the relationship appears to be quadratic based on the residual plot.\n\nquadratic_boxcox_model &lt;- lm(mpg^-0.5~horsepower+I(horsepower**2), data = auto_data)\npar(mfrow = c(2,2))\nplot(quadratic_boxcox_model)\n\n\n\n\nNote that the severity of the linearity assumption violation has been reduced, while the homoscedasticity and normality of errors assumptions seem to be satisfied. Note that transformation of the predictor \\(X\\) doesn’t change the distribution of errors. Thus, it was expected that the transformation on \\(X\\) will not deteriorate the model in terms of the assumptions regarding the distribution of error terms.\nLet us confirm our intuition based on diagnostic plots with statistical tests.\n\nbptest(quadratic_boxcox_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  quadratic_boxcox_model\nBP = 0.96746, df = 2, p-value = 0.6165\n\n\nAs expected, the errors are homoscedastic.\n\nshapiro.test(quadratic_boxcox_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  quadratic_boxcox_model$residuals\nW = 0.98943, p-value = 0.006297\n\n\nThere is a slight deviation from the normal distribution, but its only due to three points as we saw earlier.\n\nanova(quadratic_boxcox_model, power_full_model)\n\nAnalysis of Variance Table\n\nModel 1: mpg^-0.5 ~ horsepower + I(horsepower^2)\nModel 2: mpg^-0.5 ~ as.factor(horsepower)\n  Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    389 0.138224                                  \n2    299 0.085073 90  0.053151 2.0756 2.372e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model fails the linearity assumption test. But as noted earlier, this failure is due to the presence of very few or no replicates at some points. This potentially results in the full model explaining the noise, which in turn results in the model failing the linearity test. Again, we can bin the predictor values to have replicates for every bin, and then the model is likely to satisfy the linearity assumption.\nAs shown in the plot below, the full model is overfitting the data, and explaining some noise due to the absence of replicates for some values of horsepower.\n\nggplot(data = auto_data, aes(x = horsepower, y = mpg))+\n  geom_point()+\n  geom_line(aes(y = power_full_model$fitted.values^-2), color = \"blue\")\n\n\n\n\nTo resolve this issue, we will bin the predictor values, so that we have replicates for each distinct value of the predictor, and get a better fit for the full model\n\n#Binning horsepower into 20 equal width bins\nhorsepower_bins &lt;- cut(auto_data$horsepower, breaks = 20)\n\n\n# Full model based on binned horsepower\npower_full_model_binned_hp &lt;- lm(mpg^-0.5~horsepower_bins, data = auto_data)\nanova(quadratic_boxcox_model, power_full_model_binned_hp)\n\nAnalysis of Variance Table\n\nModel 1: mpg^-0.5 ~ horsepower + I(horsepower^2)\nModel 2: mpg^-0.5 ~ horsepower_bins\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    389 0.13822                              \n2    372 0.12912 17 0.0091085 1.5437 0.07696 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that now the quadratic_boxcox_model model saitisfies the linearity assumption (assuming a significance level of 5%).\nNote that our full model based on the binned horsepower values seems to better fit the data as shown in the plot below.\n\ncolors &lt;- c(\"Full model (binned predictor)\" = \"red\", \"Quadratic Box-Cox model\" = \"blue\")\nggplot(data = auto_data, aes(x = horsepower, y = mpg))+\n  geom_point()+\n  geom_line(aes(y = quadratic_boxcox_model$fitted.values^-2, color = \"Quadratic Box-Cox model\"))+\n  geom_line(aes(y = power_full_model_binned_hp$fitted.values^-2, color = \"Full model (binned predictor)\"))+\n    scale_color_manual(values = colors)+\n  theme(legend.title = element_blank(),\n        legend.position = c(0.75,0.85))"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#loading-libraries",
    "href": "Diagnostics_Remedial_Measures_MLR.html#loading-libraries",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.1 Loading libraries",
    "text": "5.1 Loading libraries\n\nlibrary(ggplot2) #for making plots\nlibrary(lmtest) #for bptest()\nlibrary(MASS) #for boxcox()\n#library(rgl) #for 3D plots"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#reading-data",
    "href": "Diagnostics_Remedial_Measures_MLR.html#reading-data",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.2 Reading data",
    "text": "5.2 Reading data\n\nsales_data &lt;- read.csv('./Datasets/Advertising.csv')"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#developing-model",
    "href": "Diagnostics_Remedial_Measures_MLR.html#developing-model",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.3 Developing model",
    "text": "5.3 Developing model\n\nlinear_model &lt;- lm(sales~TV+radio, data = sales_data)"
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#model-fit",
    "href": "Diagnostics_Remedial_Measures_MLR.html#model-fit",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.4 Model fit",
    "text": "5.4 Model fit\nWith two predictors, the model fit can be visualized via a 3-dimensional plot (2 predictors on 2 axes, and the response on the 3rd axis). However, with more than 2 predictors it will be impossible to visualize the model fit based on all the predictors and response. Thus, visualizing the response with the predictors is not an effective strategy. We will visualize the model fit via a 3D plot for this example since we have only 2 predictors, but that will be just for seeing the effectiveness of our approach of performing diagnostic tests and using remedial measure to develop a MLR model. We will not use the visualization of the response with the predictors to propose a model development strategy as such a strategy will fail for more than 2 predictors.\nLet us check the goodness-of-fit of the model.\n\nsummary(linear_model)\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = sales_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nThe model \\(R^2_{adj}\\) is 90% indicating a good-fit of the model to the data."
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#diagnostic-plots-statistical-tests",
    "href": "Diagnostics_Remedial_Measures_MLR.html#diagnostic-plots-statistical-tests",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.5 Diagnostic plots & statistical tests",
    "text": "5.5 Diagnostic plots & statistical tests\nLet us make diagnostic plots that help us check the model assumptions.\n\npar(mfrow = c(2,2))\nplot(linear_model)\n\n\n\n\nFrom the plot residuals vs fitted values, we see that the relationship does not seem to be linear. However, the homoscedasticity assumption seems to be only mildly violated.\nFrom the Q-Q plot, we can see that the error distribution is left-skewed, and thus the assumption of normal distribution of errors is also violated.\nLet us verify our visual check with statistical tests.\n\nbptest(linear_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  linear_model\nBP = 4.8093, df = 2, p-value = 0.0903\n\nshapiro.test(linear_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  linear_model$residuals\nW = 0.91804, p-value = 4.19e-09\n\n\nConsidering a significance level of 5%, the model does not satisfy the assumption of normal distribution of errors, but satisfies the homoscedasticity assumption.\nNote that the statistical test for the linearity assumption is not possible since we don’t have replicates for the combination of values of TV and radio expenditures as shown below.\n\nnrow(unique(sales_data[,c('TV', 'radio')]))\n\n[1] 200\n\n\nThe data has 200 observations, and each of them is a unique combination of expenditure on TV and radio. Thus, we’ll only use the diagnostic plots to check the linearity assumption.\nIn multiple linear regression, we also develop the residual plots against each predictor. These plots may indicate the predictor(s) that may not have linear relationship with the response.\n\n# Residuals vs TV expenditure\nggplot(data = sales_data, aes(x = TV, y = linear_model$residuals))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe relationship between TV expenditure and sales seems to be slightly non-linear. The constant error variance assumption is also violated.\n\n# Residuals vs radio expenditure\nggplot(data = sales_data, aes(x = radio, y = linear_model$residuals))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe relationship between radio expenditure and sales seems to be linear. However, the constant error variance assumption is violated."
  },
  {
    "objectID": "Diagnostics_Remedial_Measures_MLR.html#remedial-measures",
    "href": "Diagnostics_Remedial_Measures_MLR.html#remedial-measures",
    "title": "5  Diagnostics and Remedial measures (MLR)",
    "section": "5.6 Remedial measures",
    "text": "5.6 Remedial measures\nBased on the diagnostic plots, the linearity assumption and the assumptions regarding error distribution are violated. Since both linearity and error distribution assumptions are violated, we will recommend transforming the response sales instead of the predictors. Transforming sales is likely to change the nature of relationship between the response and the predictors, as well as the error distribution.\nLet us use the Box-Cox procedure to identify the appropriate transformation of the response sales.\n\npar(mfrow=c(1,1))\nboxcox(linear_model)\n\n\n\n\nFrom the above plot, we observe that the Box-Cox procedure suggests no transformation of the response. However, as the residual distribution is left-skewed, a convex transformation of the response was expected.\nNote that the Box-Cox procedure does not guarantee the appropriate value of the power transformation \\(\\lambda\\) to transform the residual distribution to normal. It estimates the value of \\(\\lambda\\) by maximizing the likelihood of the data based on the assumption of normal distribution of errors. Thus, it is likely to estimate the value of \\(\\lambda\\) that pushes the distribution of the residuals closer to the normal distribution. However, since we have finite data, there may exist other distributions that maximize the likelihood of the data by chance, resulting in an inappropriate value of \\(\\lambda\\).\nAs we can see that a convex transformation will be appropriate in this case, let us transform sales with one of the most popular and simple convex transformation, which is the polynomial transformation of degree 2.\n\ntransformed_resp_model &lt;- lm(sales^2 ~ TV+radio, data = sales_data)\nsummary(transformed_resp_model)\n\n\nCall:\nlm(formula = sales^2 ~ TV + radio, data = sales_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-150.040  -41.080   -5.875   36.813  177.224 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -112.08467   10.66220  -10.51   &lt;2e-16 ***\nTV             1.30461    0.05034   25.92   &lt;2e-16 ***\nradio          6.18844    0.29109   21.26   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.87 on 197 degrees of freedom\nMultiple R-squared:  0.8577,    Adjusted R-squared:  0.8563 \nF-statistic: 593.8 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nThe transformed model with \\(R^2_{adj} = 86\\%\\) seems to have a slightly worse goodness-of-fit as compared to the previous model. Let us make diagnostic plots to check model assumptions.\n\npar(mfrow=c(2,2))\nplot(transformed_resp_model)\n\n\n\n\nWe observe that there is an even stronger departure from the linear relationship assumption as compared to the previous model. However, the error distribution seems to be normal. There may be slight departure from the homoscedasticity assumption. Let us verify the homoscedasticity and normality assumptions with statistical tests.\n\nbptest(transformed_resp_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  transformed_resp_model\nBP = 5.2091, df = 2, p-value = 0.07394\n\nshapiro.test(transformed_resp_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  transformed_resp_model$residuals\nW = 0.99123, p-value = 0.2681\n\n\nBased on the above tests, the model satisfies both the homoscedasticity and the assumption of normal distribution of errors.\nTransforming the predictors may help make the relationship linear without changing the distribution of errors, which is what we need.\nLet us visualize the residuals against each predictor to identify the predictor having a non-linear relationship with the response.\n\n# Residuals vs TV expenditure\nggplot(data = sales_data, aes(x = TV, y = transformed_resp_model$residuals))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe relationship between TV expenditure and sales seems to be close to linear. Thus, transforming TV may not be appropriate.\n\n# Residuals vs radio expenditure\nggplot(data = sales_data, aes(x = radio, y = transformed_resp_model$residuals))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe relationship between radio expenditure and sales seems to be close to linear. Thus, transforming radio may not be appropriate.\nOur current model is additive, i.e., the expected change in the response sales with a unit change in either predictor is independent of the value of the other predictor. However, this assumption may not be correct. For example, the expected change in the response sales with a unit change in TV expenditure may depend on the radio expenditure. In other words, the predictors TV and radio may interact to effect the response. If they are indeed interaction, then it can be observed in the plot of residuals against the interaction.\n\n# Residuals vs TV*radio expenditure\nggplot(data = sales_data, aes(x = radio*TV, y = transformed_resp_model$residuals))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nWe can see that the residuals seem to have a relationship with the interaction of TV and radio expenditures. There seems to be curvilinear relationship between the residuals and the interaction of radio and TV. Thus, we will add the TV*radio as well as \\(`(TV*radio)^2`\\) in the model. However, note that it will be better to first only add TV*radio in the model, check the diagnostic plots, and if it doesn’t suffice, then consider addin \\(`(TV*radio)^2`\\). This is because a simpler model, if sufficient, will always be preferred over a more complex model.\n\ninteraction_model &lt;- lm(sales^2 ~ TV+radio+TV*radio+I((TV*radio)^2), data = sales_data)\nsummary(interaction_model)\n\n\nCall:\nlm(formula = sales^2 ~ TV + radio + TV * radio + I((TV * radio)^2), \n    data = sales_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.472  -7.057   0.722   8.399  46.748 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.461e+01  3.578e+00  12.466   &lt;2e-16 ***\nTV                2.835e-01  2.395e-02  11.836   &lt;2e-16 ***\nradio             5.850e-02  1.405e-01   0.416    0.678    \nI((TV * radio)^2) 9.155e-07  9.534e-08   9.602   &lt;2e-16 ***\nTV:radio          3.284e-02  1.538e-03  21.347   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.5 on 195 degrees of freedom\nMultiple R-squared:  0.9931,    Adjusted R-squared:  0.9929 \nF-statistic:  6987 on 4 and 195 DF,  p-value: &lt; 2.2e-16\n\n\nNote that \\(R^2_{adj}\\) of the model with interaction terms is 99%, which indicates a much higher goodness-of-fit as compared to the previous models. However, this need not be a good news as the model may be over-fitting, i.e., fitting on the noise in the data.\nAlong with testing the model assumptions, we should also test the model accuracy on unseen data, to check for potential overfitting. This can be done by examining the cross-validated error. However, we will defer this topic to a later chapter.\nLet us make the diagnostic plots.\n\npar(mfrow=c(2,2))\nplot(interaction_model)\n\n\n\n\nNote that the linearity assumption seems to be satisfied by the model with the interaction terms. There seems to be only a slight departure from the homoscedasticity and the normal distribution of errors assumptions. As we have transformed the predictors, the distribution of errors is not expected to change much.\nLet us verify the assumptions based on statistical tests.\n\nbptest(interaction_model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  interaction_model\nBP = 17.163, df = 4, p-value = 0.001797\n\nshapiro.test(interaction_model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  interaction_model$residuals\nW = 0.98616, p-value = 0.04772\n\n\nNote that there is no strong departure from the homoscedasticity and normal distribution of errors assumptions. If we consider a significance level of 0.1%, the model satisfies both the assumptions. The linear relationship assumption seems to be satisfied based on the diagnostic plot.\nIn this example, we have used diagnostic plots and intuition to develop a model that satisfies the assumptions. Note that intuition may not work in all cases, and we’ll study more remedial measures in later chapters to address model assumption violations."
  },
  {
    "objectID": "multicollinearity.html#why-and-how-is-collinearity-a-problem",
    "href": "multicollinearity.html#why-and-how-is-collinearity-a-problem",
    "title": "6  Multicollinearity",
    "section": "6.1 Why and how is collinearity a problem?",
    "text": "6.1 Why and how is collinearity a problem?\n(Source: page 100-101 of ISLR)\nThe presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for \\(\\hat \\beta_j\\) to grow. Recall that the t-statistic for each predictor is calculated by dividing \\(\\hat \\beta_j\\) by its standard error. Consequently, collinearity results in a decline in the \\(t\\)-statistic. As a result, in the presence of collinearity, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity."
  },
  {
    "objectID": "multicollinearity.html#how-to-measure-collinearitymulticollinearity",
    "href": "multicollinearity.html#how-to-measure-collinearitymulticollinearity",
    "title": "6  Multicollinearity",
    "section": "6.2 How to measure collinearity/multicollinearity?",
    "text": "6.2 How to measure collinearity/multicollinearity?\n(Source: page 102 of ISLR)\nUnfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor the ratio of the variance of \\(\\hat \\beta_j\\) when fitting the full model divided by the variance of \\(\\hat \\beta_j\\) if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\nThe estimated variance of the coefficient \\(\\beta_j\\), of the \\(j^{th}\\) predictor \\(X_j\\), can be expressed as:\n\\[\\hat{var}(\\hat{\\beta_j}) = \\frac{(\\hat{\\sigma})^2}{(n-1)\\hat{var}({X_j})}.\\frac{1}{1-R^2_{X_j|X_{-j}}},\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the \\(R\\)-squared for the regression of \\(X_j\\) on the other covariates (a regression that does not involve the response variable \\(Y\\)).\nIn case of simple linear regression, the variance expression in the equation above does not contain the term \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\), as there is only one predictor. However, in case of multiple linear regression, the variance of the estimate of the \\(j^{th}\\) coefficient (\\(\\hat{\\beta_j}\\)) gets inflated by a factor of \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\) (Note that in the complete absence of collinearity, \\(R^2_{X_j|X_{-j}}=0\\), and the value of this factor will be 1).\nThus, the Variance inflation factor, or the VIF for the estimated coefficient of the \\(j^{th}\\) predictor \\(X_j\\) is:\nVIF\\((\\hat \\beta_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\)\nLet us consider the dataset Credit.csv. We’ll compute VIF of the predictors when predicting the credit card balance of customers.\n\n#loading library for VIF\nlibrary(car)\n\n\n# Reading data\ncredit_data &lt;- read.csv('./Datasets/Credit.csv')\nhead(credit_data)\n\n   Income Limit Rating Cards Age Education Own Student Married Region Balance\n1  14.891  3606    283     2  34        11  No      No     Yes  South     333\n2 106.025  6645    483     3  82        15 Yes     Yes     Yes   West     903\n3 104.593  7075    514     4  71        11  No      No      No   West     580\n4 148.924  9504    681     3  36        11 Yes      No      No   West     964\n5  55.882  4897    357     2  68        16  No      No     Yes  South     331\n6  80.180  8047    569     4  77        10  No      No      No  South    1151\n\n# Developing model\nmodel &lt;- lm(Balance~Limit+Rating+Cards+Age, data = credit_data)\nsummary(model)\n\n\nCall:\nlm(formula = Balance ~ Limit + Rating + Cards + Age, data = credit_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-710.90 -139.59  -13.04  133.87  829.10 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -270.37864   55.78092  -4.847  1.8e-06 ***\nLimit          0.11229    0.07458   1.506 0.132993    \nRating         0.91305    1.11425   0.819 0.413034    \nCards         22.85955    9.92792   2.303 0.021823 *  \nAge           -2.38988    0.66529  -3.592 0.000369 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 227.8 on 395 degrees of freedom\nMultiple R-squared:  0.7569,    Adjusted R-squared:  0.7544 \nF-statistic: 307.4 on 4 and 395 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the coefficients of Limit and Rating are statistically insignificant at 5% significance level.\n\n#Computing VIF\nvif(model)\n\n     Limit     Rating      Cards        Age \n227.787786 228.437806   1.424475   1.012225 \n\n\nNote that the VIF of Income and Rating is very high indicating presence of multicollinearity. This may have lead to a high standard error for the coefficients of these predictors, thereby making them appear statistically insignificant.\nLet us remove Rating from the model as it seems to be redundant based on its high VIF.\n\nmodel &lt;- lm(Balance~Limit+Cards+Age, data = credit_data)\nsummary(model)\n\n\nCall:\nlm(formula = Balance ~ Limit + Cards + Age, data = credit_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-698.13 -135.29  -14.94  133.38  809.35 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.486e+02  4.899e+01  -5.074 6.01e-07 ***\nLimit        1.733e-01  4.965e-03  34.897  &lt; 2e-16 ***\nCards        2.729e+01  8.323e+00   3.279  0.00113 ** \nAge         -2.383e+00  6.650e-01  -3.584  0.00038 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 227.8 on 396 degrees of freedom\nMultiple R-squared:  0.7565,    Adjusted R-squared:  0.7546 \nF-statistic:   410 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\nNote that now the coefficient of Limit is highly statistically significant. This seems to be due to the reduction in standard error of the coefficient, which is potentially due to the reduction in multicollinearity from the model.\nLet us re-check the VIF of the predictors of the updated model to see if multicollinearity has indeed been reduced.\n\nvif(model)\n\n   Limit    Cards      Age \n1.010319 1.001883 1.012080 \n\n\nThe model no longer suffers from multicollinearity. This is an example that shows that we may make incorrect inference in the presence of multicollinearity.\nNote that the \\(R^2_{adj}\\) of the updated model is almost the same as that of the previous model. Thus, the goodness-of-fit of the model seems to be unaffected by multicollinearity. However, this need not always be the case."
  },
  {
    "objectID": "multicollinearity.html#addressing-multicollinearity-in-polynomial-models",
    "href": "multicollinearity.html#addressing-multicollinearity-in-polynomial-models",
    "title": "6  Multicollinearity",
    "section": "6.3 Addressing multicollinearity in polynomial models",
    "text": "6.3 Addressing multicollinearity in polynomial models\nIn polynomial models, it is likely that \\(X\\) will be highly correlated with \\(X^p\\), resulting in imprecise estimates of the regression coefficents. Let us consider the following simulation example.\n\n# Simulating data\nX &lt;- seq(5,8,0.01)\ny &lt;- X + X^2 + rnorm(length(X))\n\nmodel &lt;- lm(y~X+I(X^2))\nsummary(model)\n\n\nCall:\nlm(formula = y ~ X + I(X^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1373 -0.6224  0.0794  0.6475  1.9272 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.96900    3.50413   1.133    0.258    \nX           -0.24369    1.09157  -0.223    0.823    \nI(X^2)       1.09464    0.08382  13.060   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.982 on 298 degrees of freedom\nMultiple R-squared:  0.9936,    Adjusted R-squared:  0.9936 \nF-statistic: 2.314e+04 on 2 and 298 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the \\(X\\) appears to be statistically insignificant in the model. This may be due to the presence of multicollinearity. Let us compute the correlation between \\(X\\) and \\(X^2\\).\n\ncor(X, X^2)\n\n[1] 0.9982179\n\n\nThe high correlation indicates presence of multicollineairty. Multicollinearity can also be observed with the high value of VIF as shown below.\n\nvif(model)\n\n       X   I(X^2) \n280.8106 280.8106 \n\n\nCentering the predictors may help reduce multicollinearity as shown below\n\nx &lt;- X-mean(X)\ncor(x,x^2)\n\n[1] 9.0533e-17\n\n\nThe correlation between \\(X\\) and \\(X^2\\), after centering the predictor, is almost 0.\n\nmodel_centered &lt;- lm(y~x+I(x^2))\nvif(model_centered)\n\n     x I(x^2) \n     1      1 \n\n\nNote that multicollinearity has been eliminated in the model with centered predictor.\nAlso, the coefficients of both \\(X\\) and \\(X^2\\) are statistically significant (see model summary below) as they should be as per the true model.\n\nsummary(model_centered)\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1373 -0.6224  0.0794  0.6475  1.9272 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.63337    0.08490  572.82   &lt;2e-16 ***\nx           13.98658    0.06514  214.72   &lt;2e-16 ***\nI(x^2)       1.09464    0.08382   13.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.982 on 298 degrees of freedom\nMultiple R-squared:  0.9936,    Adjusted R-squared:  0.9936 \nF-statistic: 2.314e+04 on 2 and 298 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "multicollinearity.html#when-can-we-overlook-multicollinearity",
    "href": "multicollinearity.html#when-can-we-overlook-multicollinearity",
    "title": "6  Multicollinearity",
    "section": "6.4 When can we overlook multicollinearity?",
    "text": "6.4 When can we overlook multicollinearity?\n\nThe severity of the problems increases with the degree of the multicollinearity. Therefore, if there is only moderate multicollinearity (5 &lt; VIF &lt; 10), we may overlook it.\nMulticollinearity affects only the standard errors of the coefficients of collinear predictors. Therefore, if multicollinearity is not present for the predictors that we are particularly interested in, we may not need to resolve it.\nMulticollinearity affects the standard error of the coefficients and thereby their \\(p\\)-values, but in general, it does not influence the prediction accuracy, except in the case that the coefficients are so unstable that the predictions are outside of the domain space of the response. If our sole aim is prediction, and we don’t wish to infer the statistical significance of predictors, then we may avoid addressing multicollinearity. “The fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations, provided these inferences are made within the region of observations” - Neter, John, Michael H. Kutner, Christopher J. Nachtsheim, and William Wasserman. “Applied linear statistical models.” (1996): 318."
  },
  {
    "objectID": "quant_qual_preds.html#interaction-model",
    "href": "quant_qual_preds.html#interaction-model",
    "title": "7  Variable interactions",
    "section": "7.1 Interaction model",
    "text": "7.1 Interaction model\nLet us consider the file car_data.csv.\n\n# Reading data\ncar_data &lt;- read.csv('./Datasets/car_data.csv')\nhead(car_data)\n\n  carID  brand        model year transmission mileage fuelType tax     mpg\n1 12002 hyundi     Santa Fe 2017    Semi-Auto   32467   Diesel 235 42.9709\n2 12003     vw       Arteon 2019    Automatic    1555   Petrol 145 40.5071\n3 12005 toyota        Verso 2003    Automatic  104000   Petrol 300 34.5227\n4 12006   ford  Grand C-MAX 2018       Manual    5113   Petrol 145 47.6225\n5 12007    bmw           X6 2019    Automatic    9010   Diesel 145 35.2224\n6 12008 toyota        Prius 2016    Automatic   32853   Hybrid  10 63.8371\n  engineSize price\n1        2.2 18991\n2        1.5 22500\n3        1.8  2395\n4        1.0 14000\n5        3.0 58700\n6        1.8 22995\n\n\nIn an additive model, we assume that the association between a predictor \\(X_j\\) and response \\(Y\\) does not depend on the value of other predictors. For example, consider the multiple linear regression model below.\n\n# Additive model\nadditive_model &lt;- lm(price~year+engineSize+mileage+mpg, data = car_data)\nsummary(additive_model)\n\n\nCall:\nlm(formula = price ~ year + engineSize + mileage + mpg, data = car_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-35346  -5131  -1605   2854  87509 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.661e+06  1.489e+05 -24.593   &lt;2e-16 ***\nyear         1.818e+03  7.375e+01  24.647   &lt;2e-16 ***\nengineSize   1.218e+04  1.900e+02  64.107   &lt;2e-16 ***\nmileage     -1.474e-01  8.768e-03 -16.817   &lt;2e-16 ***\nmpg         -7.931e+01  9.338e+00  -8.493   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9564 on 4955 degrees of freedom\nMultiple R-squared:  0.6605,    Adjusted R-squared:  0.6602 \nF-statistic:  2410 on 4 and 4955 DF,  p-value: &lt; 2.2e-16\n\n\nThe above model assumes that the average increase in price associated with a unit increase in engineSize is always $12,180, regardless of the value of other predictors. However, this assumption may be incorrect."
  },
  {
    "objectID": "quant_qual_preds.html#interaction-between-continuous-predictors",
    "href": "quant_qual_preds.html#interaction-between-continuous-predictors",
    "title": "7  Variable interactions",
    "section": "7.2 Interaction between continuous predictors",
    "text": "7.2 Interaction between continuous predictors\nWe can relax this assumption by considering another predictor, called an interaction term. Let us assume that the average increase in price associated with a one-unit increase in engineSize depends on the model year of the car. In other words, there is an interaction between engineSize and year. This interaction can be included as a predictor, which is the product of engineSize and year. Note that there are several possible interactions that we can consider. Here the interaction between engineSize and year is just an example.\n\n# Interaction model\ninteraction_model &lt;- lm(price~year*engineSize+mileage+mpg, data = car_data)\nsummary(interaction_model)\n\n\nCall:\nlm(formula = price ~ year * engineSize + mileage + mpg, data = car_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40479  -4929  -1548   2864  85271 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      5.606e+05  2.737e+05   2.048   0.0406 *  \nyear            -2.754e+02  1.357e+02  -2.029   0.0425 *  \nengineSize      -1.796e+06  9.968e+04 -18.019   &lt;2e-16 ***\nmileage         -1.525e-01  8.496e-03 -17.954   &lt;2e-16 ***\nmpg             -8.434e+01  9.048e+00  -9.322   &lt;2e-16 ***\nyear:engineSize  8.968e+02  4.943e+01  18.142   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9262 on 4954 degrees of freedom\nMultiple R-squared:  0.6816,    Adjusted R-squared:  0.6813 \nF-statistic:  2121 on 5 and 4954 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the \\(R^2\\) has increased as compared to the additive model, since we added a predictor.\nThe model equation is:\nprice = \\(\\beta_0\\) + \\(\\beta_1\\)year + \\(\\beta_2\\)engineSize + \\(\\beta_3\\)(year \\(\\times\\) engineSize) + \\(\\beta_4\\)mileage + \\(\\beta_5\\)mpg, or\nprice = \\(\\beta_0\\) + \\(\\beta_1\\)year + (\\(\\beta_2+\\beta_3\\)year) \\(\\times\\) engineSize + \\(\\beta_4\\)mileage + \\(\\beta_5\\)mpg, or\nprice = \\(\\beta_0 + \\beta_1\\)year + \\(\\tilde \\beta\\)engineSize + \\(\\beta_4\\)mileage + \\(\\beta_5\\)mpg,\nSince \\(\\tilde \\beta\\) is a function of year, the association between engineSize and price is no longer a constant. A change in the value of year will change the association between price and engineSize.\nSubstituting the values of the coefficients:\nprice = 5.606e5 - 275.3833year + (-1.796e6+896.7687year)engineSize -0.1525mileage -84.3417mpg\nThus, for cars launched in the year 2010, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687 * 2010 \\(\\approx\\) $6,500, assuming all the other predictors are constant. However, for cars launched in the year 2020, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687*2020 \\(\\approx\\) $15,500 , assuming all the other predictors are constant.\nSimilarly, the equation can be re-arranged as:\nprice = 5.606e5 +(-275.3833+896.7687engineSize)year -1.796e6engineSize -0.1525mileage -84.3417mpg\nThus, for cars with an engine size of 2 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 2 \\(\\approx\\) $1500, assuming all the other predictors are constant. However, for cars with an engine size of 3 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 3 \\(\\approx\\) $2400, assuming all the other predictors are constant."
  },
  {
    "objectID": "quant_qual_preds.html#qualitative-predictors",
    "href": "quant_qual_preds.html#qualitative-predictors",
    "title": "7  Variable interactions",
    "section": "7.3 Qualitative predictors",
    "text": "7.3 Qualitative predictors\nLet us develop a model for predicting price based on engineSize and the qualitative predictor transmission.\n\n#checking the distribution of values of transmission\ntable(car_data$transmission)\n\n\nAutomatic    Manual     Other Semi-Auto \n     1660      1948         1      1351 \n\n\nNote that the Other category of the variable transmission contains only a single observation, which is likely to be insufficient to train the model. We’ll remove that observation from the car data. Another option may be to combine the observation in the Other category with the nearest category, and keep it in the data.\n\ncar_data = car_data[car_data$transmission!='Other',]\n\n\nqual_pred_model &lt;- lm(price~engineSize+transmission, data = car_data)\nsummary(qual_pred_model)\n\n\nCall:\nlm(formula = price ~ engineSize + transmission, data = car_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47181  -6726  -1145   5204  95998 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             3042.7      661.2   4.602 4.29e-06 ***\nengineSize             10226.8      247.5  41.323  &lt; 2e-16 ***\ntransmissionManual     -6770.6      442.1 -15.314  &lt; 2e-16 ***\ntransmissionSemi-Auto   4994.3      443.0  11.274  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12080 on 4955 degrees of freedom\nMultiple R-squared:  0.4587,    Adjusted R-squared:  0.4584 \nF-statistic:  1400 on 3 and 4955 DF,  p-value: &lt; 2.2e-16\n\n\nNote that there is no coefficient for the Automatic level of the variable Transmission. If a car doesn’t have Manual or Semi-Automatic transmission, then it has an Automatic transmission. Thus, the coefficient of Automatic will be redundant, and the dummy variable corresponding to Automatic transmission is dropped from the model.\nThe level of the categorical variable that is dropped from the model is called the baseline level. Here Automatic transmission is the baseline level. The coefficients of other levels of transmission should be interpreted with respect to the baseline level.\nQ: Interpret the intercept term.\nAns: For the hypothetical scenario of a car with zero engine size and Automatic transmission, the estimated mean car price is \\(\\approx\\) $3042.\nQ: Interpret the coefficient of transmissionManual.\nAns: The estimated mean price of a car with manual transmission is \\(\\approx\\) $6770 less than that of a car with Automatic transmission.\nLet us visualize the developed model.\n\ncolors &lt;- c(\"Automatic\" = \"red\", \"Manual\" = \"blue\", \"Semi-Automatic\" = \"green\")\n\ncoefs &lt;- qual_pred_model$coefficients\nx &lt;- car_data$engineSize\nggplot(data = car_data, aes(x = engineSize))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize'], color = 'Automatic'))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize']+coefs['transmissionManual'], color = 'Manual'))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize']+coefs['transmissionSemi-Auto'], color = 'Semi-Automatic'))+\n  theme(legend.title = element_blank(),\n        legend.position = c(0.15,0.85))+\n  labs(\n    y = 'Predicted car price',\n    x = 'Engine size (in litre)'\n  )\n\n\n\n\nBased on the developed model, for a given engine size, the car with a semi-automatic transmission is estimated to be the most expensive on average, while the car with a manual transmission is estimated to be the least expensive on average."
  },
  {
    "objectID": "quant_qual_preds.html#interaction-between-qualitative-and-continuous-predictors",
    "href": "quant_qual_preds.html#interaction-between-qualitative-and-continuous-predictors",
    "title": "7  Variable interactions",
    "section": "7.4 Interaction between qualitative and continuous predictors",
    "text": "7.4 Interaction between qualitative and continuous predictors\nNote that the qualitative predictor leads to fitting 3 parallel lines to the data, as there are 3 categories.\nHowever, note that we have made the constant association assumption. The fact that the lines are parallel means that the average increase in car price for one litre increase in engine size does not depend on the type of transmission. This represents a potentially serious limitation of the model, since in fact a change in engine size may have a very different association on the price of an automatic car versus a semi-automatic or manual car.\nThis limitation can be addressed by adding an interaction variable, which is the product of engineSize and the dummy variables for semi-automatic and manual transmissions.\n\nqual_pred_int_model &lt;- lm(price~engineSize*transmission, data = car_data)\nsummary(qual_pred_int_model)\n\n\nCall:\nlm(formula = price ~ engineSize * transmission, data = car_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-56431  -6453  -1033   5184  96479 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        3754.7      895.2   4.194 2.79e-05 ***\nengineSize                         9928.6      354.5  28.006  &lt; 2e-16 ***\ntransmissionManual                 1768.6     1294.1   1.367 0.171786    \ntransmissionSemi-Auto             -5282.7     1416.5  -3.729 0.000194 ***\nengineSize:transmissionManual     -5285.9      646.2  -8.180 3.57e-16 ***\nengineSize:transmissionSemi-Auto   4162.2      552.6   7.532 5.90e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11850 on 4953 degrees of freedom\nMultiple R-squared:  0.4788,    Adjusted R-squared:  0.4782 \nF-statistic: 909.9 on 5 and 4953 DF,  p-value: &lt; 2.2e-16\n\n\nThe model equation for the model with interactions is:\nAutomatic transmission: price = 3754.7238 + 9928.6082engineSize,\nSemi-Automatic transmission: price = 3754.7238 + 9928.6082engineSize + (-5282.7164+4162.2428engineSize),\nManual transmission: price = 3754.7238 + 9928.6082engineSize +(1768.5856-5285.9059engineSize), or\nAutomatic transmission: price = 3754.7238 + 9928.6082engineSize,\nSemi-Automatic transmission: price = -1527 + 7046engineSize,\nManual transmission: price = 5523 + 4642engineSize.\nQ: Interpret the coefficient of manual tranmission, i.e., the coefficient of transmissionManual.\nA: For the hypothetical scenario of a car with zero engine size,the estimated mean price of a car with Manual transmission is \\(\\approx\\) $1768 more than the estimated mean price of a car with Automatic transmission.\nQ: Interpret the coefficient of the interaction between engine size and manual transmission, i.e., the coefficient of engineSize:transmissionManual.\nA: For a unit (or a litre) increase in engineSize , the increase in estimated mean price of a car with Manual transmission is \\(\\approx\\) $5285 less than the increase in estimated mean price of a car with Automatic transmission.\n\ncolors &lt;- c(\"Automatic\" = \"red\", \"Manual\" = \"blue\", \"Semi-Automatic\" = \"green\")\n\ncoefs &lt;- qual_pred_int_model$coefficients\nx &lt;- car_data$engineSize\nggplot(data = car_data, aes(x = engineSize))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize'], color = 'Automatic'))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize']+coefs['transmissionManual']+x*coefs['engineSize:transmissionManual'], color = 'Manual'))+\n  geom_line(aes(y = coefs['(Intercept)']+x*coefs['engineSize']+coefs['transmissionSemi-Auto']+x*coefs['engineSize:transmissionSemi-Auto'], color = 'Semi-Automatic'))+\n  theme(legend.title = element_blank(),\n        legend.position = c(0.15,0.85))+\n  labs(\n    y = 'Predicted car price',\n    x = 'Engine size (in litre)'\n  )\n\n\n\n\nNote the interaction term adds flexibility to the model.\nThe slope of the regression line for semi-automatic cars is the largest. This suggests that increase in engine size is associated with a higher increase in car price for semi-automatic cars, as compared to other cars."
  },
  {
    "objectID": "Assignment A.html#instructions",
    "href": "Assignment A.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nMake R code chunks to insert code and type your answer outside the code chunks. Ensure that the solution is written neatly enough to understand and grade.\nRender the file as HTML to submit. For theoretical questions, you can either type the answer and include the solutions in this file, or write the solution on paper, scan and submit separately.\nThe assignment is worth 100 points, and is due on 7th October 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (the theory part may be scanned and submitted separately) (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.). There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\nFinal answers of each question are written clearly (1 pt).\nThe proofs are legible, and clearly written with reasoning provided for every step. They are easy to follow and understand (1 pt)"
  },
  {
    "objectID": "Assignment A.html#section",
    "href": "Assignment A.html#section",
    "title": "Appendix A — Assignment A",
    "section": "A.1 ",
    "text": "A.1 \nThe first step in using the capital asset pricing model (CAPM) is to estimate the stock’s beta \\((\\beta)\\) using the market model. The market model can be written as:\n\\(R_{it} = \\alpha_i + \\beta_iR_{mt} + \\epsilon_{it},\\)\nwhere \\(R_{it}\\) is the excess return for security \\(i\\) at time \\(t\\), \\(R_{mt}\\) is the excess return on a proxy for the market portfolio at time \\(t\\), and \\(\\epsilon_t\\) is an iid random disturbance term. The coefficient beta in this case is the CAPM beta for security \\(i\\).\nSuppose that you had estimated \\(\\beta\\) for a stock as \\(\\hat{\\beta}=1.147\\). The standard error associated with this coefficient \\(SE(\\hat{\\beta})\\) is estimated to be 0.0548. A city analyst has told you that this security closely follows the market, but that it is no more risky, on average, than the market. This can be tested by the null hypotheses that the value of beta \\((\\beta)\\) is one. The model is estimated over 62 daily observations. Test this hypothesis against a one-sided alternative that the security is more risky than the market \\((\\beta&gt;1)\\). Consider Type 1 error \\((\\alpha)\\) as \\(1\\%\\). Write down the null and alternative hypothesis. What do you conclude?\nDoes your conclusion change if you consider the Type 1 error \\((\\alpha)\\) as \\(0.1\\%\\)?\n(4 + 1 = 5 points)"
  },
  {
    "objectID": "Assignment A.html#section-1",
    "href": "Assignment A.html#section-1",
    "title": "Appendix A — Assignment A",
    "section": "A.2 ",
    "text": "A.2 \nWhen asked to state the simple linear regression model, a students wrote it as follows: \\(E(Y_i) = \\beta_0 + \\beta_1X_1 + \\epsilon_i\\). Is this correct? Justify your answer.\n(2 points)"
  },
  {
    "objectID": "Assignment A.html#section-2",
    "href": "Assignment A.html#section-2",
    "title": "Appendix A — Assignment A",
    "section": "A.3 ",
    "text": "A.3 \nConsider the simple linear regression model below:\n\\(Y_i = \\beta_0 + \\beta_1X_1 + \\epsilon_i, i = 1,...,n\\)\nwhere:\n\\(\\beta_0 = 100, \\beta_1 = 20,\\) and \\(\\sigma^2 =5\\). The following assumptions are made for the model:\nA. \\(E(\\epsilon_i) = 0,\\)\nB. \\(Var(\\epsilon_i) = \\sigma^2,\\)\nC. \\(Cov(\\epsilon_i, \\epsilon_j)=0 \\ \\forall i, j; i\\ne j\\)\nAn observation \\(Y\\) is made for \\(X=5\\)\nCan you state the exact probability that \\(Y\\) will fall between 195 and 205? If yes, then compute the probability. If not, then state any reasonable assumption(s) you need to make to compute the probability, and then compute the probability.\n(1 + 1 + 4 = 6 points)"
  },
  {
    "objectID": "Assignment A.html#section-3",
    "href": "Assignment A.html#section-3",
    "title": "Appendix A — Assignment A",
    "section": "A.4 ",
    "text": "A.4 \nThe Toluca Company manufactures refrigeration equipment in lots of varying sizes. The dataset toluca.txt consists of of two columns - LotSize and WorkHours required to produce the lot.\nWhen asked for a point estimate of the expected work hours for lot sizes of 30 pieces, a person gave the estimate 202 because that is the mean number of WorkHours for the three observations of LotSize = 30 pieces in the dataset. Is there an issue with this approach? Explain. If there is an issue, then suggest a better approach and use it to estimate the expected work hours for lot sizes of 30 pieces.\n(2 + 2 + 4 = 8 points)"
  },
  {
    "objectID": "Assignment A.html#section-4",
    "href": "Assignment A.html#section-4",
    "title": "Appendix A — Assignment A",
    "section": "A.5 ",
    "text": "A.5 \nConsider the simple linear regression model below:\n\\(\\log(Y)=\\beta_0+\\beta_1\\log(X)+\\epsilon\\)\nInterpret the coefficient \\(\\beta_1\\), where you mention the approximate expected percentage increase in \\(Y\\) given an increase of \\(1\\%\\) in \\(X\\).\nUse the approximation: \\(\\log(1+x) = x\\) if \\(x&lt;&lt;1\\)\n(5 points)"
  },
  {
    "objectID": "Assignment A.html#section-5",
    "href": "Assignment A.html#section-5",
    "title": "Appendix A — Assignment A",
    "section": "A.6 ",
    "text": "A.6 \nThe dataset ACT_GPA consists of the GPA at the end of freshmen year (\\(Y\\)) that can be predicted from the ACT score (\\(X\\)) of students of a college.\n\nA.6.1 \nObtain the least square estimates of the regression coefficients, and error standard deviation, and state the estimated regression function.\n(5 points)\n\n\nA.6.2 \nObtain the maximum likelihood estimate of the error standard deviation. Is it the same as that obtained in the previous question? Why or why not? If it isn’t same, which estimate will you prefer - the MLE or the one obtained in the previous question and why?\n(2 + 2 + 4 = 8 points)\n\n\nA.6.3 \nInterpret the estimates of the regression coefficients and the error standard deviation as obtained in A.6.1. What is the increase in expected GPA for an increase of 2 points in the ACT score?\n(6 + 2 = 8 points)\n\n\nA.6.4 \nDoes ACT have a statistically significant relationship with the GPA? Justify your answer.\n(1 + 2 = 3 points)\n\n\nA.6.5 \nPlot the estimated regression function and the data. Does the estimated regression function appear to fit the data well?\n(3 + 1 = 4 points)\n\n\nA.6.6 \nInclude the 95% confidence and prediction intervals in the above plot.\n(6 points)\n\n\nA.6.7 \nObtain a point estimate, and the 95% confidence and prediction intervals of the freshman GPA for students with an ACT score of \\(30\\).\n(1 + 2 + 2 = 5 points)\n\n\nA.6.8 \nThe intercept of the model developed in Q4(a) is the expected GPA when the ACT score is zero. However, the ACT score can never be zero as the minimum possible ACT score is 1 (ref). So, should the intercept be removed from the model? Why or why not?\n(2 + 4 = 6 points)"
  },
  {
    "objectID": "Assignment A.html#section-14",
    "href": "Assignment A.html#section-14",
    "title": "Appendix A — Assignment A",
    "section": "A.7 ",
    "text": "A.7 \nConsider the regression model in A.3, where the parameters are estimated using Maximum likelihood estimation. Let \\(e_i\\) denote the \\(i^{th}\\) residual. Prove that:\n\n\\(\\sum_{i = 1}^n \\hat{Y}_ie_i = 0\\)\n\\(\\sum_{i=1}^n Y_i = \\sum_{i=1}^n \\hat{Y}_i\\)\nThe regression line passes through the point (\\(\\bar{X}, \\bar{Y}\\))\n\n(3 + 3 + 3 = 9 points)"
  },
  {
    "objectID": "Assignment A.html#section-15",
    "href": "Assignment A.html#section-15",
    "title": "Appendix A — Assignment A",
    "section": "A.8 ",
    "text": "A.8 \nConsider the regression model:\n\\(Y_i = \\beta_0 + \\epsilon_i, i = 1,...,n\\), where \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nDerive the maximum likelihood estimate of \\(\\beta_0\\), and show whether it is a biased or unbiased estimate of \\(\\beta_0\\).\n(3 + 3 = 6 points)"
  },
  {
    "objectID": "Assignment A.html#section-16",
    "href": "Assignment A.html#section-16",
    "title": "Appendix A — Assignment A",
    "section": "A.9 ",
    "text": "A.9 \nConsider the regression model:\n\\(Y_i = \\beta_1X_i + \\epsilon_i, i = 1,...,n\\), where \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nDerive the maximum likelihood estimate of \\(\\beta_1\\), and show whether it is a biased or unbiased estimate of \\(\\beta_1\\).\n(4 + 5 = 9 points)"
  },
  {
    "objectID": "Assignment B.html#instructions",
    "href": "Assignment B.html#instructions",
    "title": "Appendix B — Assignment B",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nMake R code chunks to insert code and type your answer outside the code chunks. Ensure that the solution is written neatly enough to understand and grade.\nRender the file as HTML to submit. For theoretical questions, you can either type the answer and include the solutions in this file, or write the solution on paper, scan and submit separately.\nThe assignment is worth 100 points, and is due on 14th October 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (the theory part may be scanned and submitted separately) (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.). There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\nFinal answers of each question are written clearly (1 pt).\nThe proofs are legible, and clearly written with reasoning provided for every step. They are easy to follow and understand (1 pt)\n\nAll question are based on the normal linear regression model below, unless otherwise stated:\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, i = 1,...,n\\), where \\(\\epsilon \\sim N(0, \\sigma^2)...(1)\\)"
  },
  {
    "objectID": "Assignment B.html#section",
    "href": "Assignment B.html#section",
    "title": "Appendix B — Assignment B",
    "section": "B.1 ",
    "text": "B.1 \nThe dataset crime.txt consists of number of crimes per 100,000 residents and percentage of individuals having high school diploma in 84 counties.\n\nB.1.1 \nEstimate the expected decrease in number of crimes per 100,000 residents when the proportion of individuals having high school diploma increases by 1%. Use a 99% confidence interval. Interpret the interval estimate.\n(4 + 2 = 6 points)\n\n\nB.1.2 \nFor what percentage of individuals having a high school diploma in a county will you be the most confident in estimating the crime rate with a confidence interval of a given width?\n(2 points)\n\n\nB.1.3 \nFor what range of values of the percentage of individuals having a high school diploma in a county will it be inappropriate to use the developed model (in B.1.1) to predict the crime rate?\n(2 points)\n\n\nB.1.4 \nPredict the number of crimes per 100,000 residents of a county using a 90% prediction interval if 75% of the individuals have a high school diploma in the county. Interpret the prediction interval.\n(2 + 2 = 4 points)\n\n\nB.1.5 \nA consultant had stated that the expected number of crimes per 100,000 should reduce by at least 1000 for a 4% increase in the proportion of people having high school diplomas. Conduct a hypothesis test to verify the statement. Consider the probability of type 1 error as 5%. State the null and alternate hypotheses, p-value, and conclusion from the test.\n(2 + 2 + 2 = 6 points)\n\n\nB.1.6 \nWhat is the probability that you will reject the null hypothesis in the previous question if it is actually false, and the true value of \\(\\beta_1\\) is \\(\\beta_1 = -200\\).\n(7 points)\n\n\nB.1.7 \nBy how much is the total variation in crime rate reduced when percentage of high school graduates is introduced into the analysis?\n(2 points)"
  },
  {
    "objectID": "Assignment B.html#section-8",
    "href": "Assignment B.html#section-8",
    "title": "Appendix B — Assignment B",
    "section": "B.2 ",
    "text": "B.2 \nSuppose that the normal error regression model is applicable except that the error variance is not constant; rather the variance is larger, the larger is \\(X\\). Does \\(\\beta_1=0\\) still imply that there is no linear association between \\(X\\) and \\(Y\\)? Explain.\nDoes it also imply that there is no association between \\(X\\) and \\(Y\\)? Or, does it also imply that there is an association between \\(X\\) and \\(Y\\)? Explain.\n(2 + 2 + 2 = 6 points)"
  },
  {
    "objectID": "Assignment B.html#section-9",
    "href": "Assignment B.html#section-9",
    "title": "Appendix B — Assignment B",
    "section": "B.3 ",
    "text": "B.3 \nShow that the regression sum of squares has only one degree of freedom for model (1). You may use the normal equations obtained from minimizing of sum of squared errors.\n(4 points)"
  },
  {
    "objectID": "Assignment B.html#section-10",
    "href": "Assignment B.html#section-10",
    "title": "Appendix B — Assignment B",
    "section": "B.4 ",
    "text": "B.4 \nIn a small-scale regression study, five observations on \\(Y\\) were obtained corresponding to \\(X=1, 4, 10, 11,\\) and \\(14\\). Assume that \\(\\sigma = 0.6, \\beta_0 = 5,\\) and \\(\\beta_1=3\\).\n\nB.4.1 \nWhat are the expected values of MSR and MSE?\n(3 + 2 = 5 points)\n\n\nB.4.2 \nFor determining whether or not a regression relation exists, would it have been better or worse to have made the five observations at \\(X=6,7,8,9,10\\)? Why? Would the same answer apply if the principal purpose were to estimate the mean response for \\(X=8\\). Explain.\n(3 + 3 = 6 points)"
  },
  {
    "objectID": "Assignment B.html#section-13",
    "href": "Assignment B.html#section-13",
    "title": "Appendix B — Assignment B",
    "section": "B.5 ",
    "text": "B.5 \nConsider the following code and its output below.\n\n#Setting seed for reproducibility\nset.seed(10)\n\n#Simulating data\nx &lt;- seq(0, 1, 0.01)\ny &lt;- 0.5 + 2*x + rnorm(length(x))\ndata &lt;- data.frame(x = x, y = y)\n\n#Developing linear regression model\nmodel &lt;- lm(y~x, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8866 -0.6294  0.0103  0.6819  2.2644 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.08838    0.18370   0.481    0.631    \nx            2.53776    0.31738   7.996 2.45e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9299 on 99 degrees of freedom\nMultiple R-squared:  0.3924,    Adjusted R-squared:  0.3863 \nF-statistic: 63.94 on 1 and 99 DF,  p-value: 2.448e-12\n\n\nWe see that the intercept is statistically insignificant. As it is insignificant, a student suggested that it won’t matter if it is removed from the developed model, i.e., if the developed model is changed to:\n\\(Y_i = \\beta_1X_i + \\epsilon_i, i = 1,...,n\\), where \\(\\epsilon \\sim N(0, \\sigma^2) ... (2)\\)\n\nB.5.1 \nGiven that \\(\\beta_0 \\ne 0\\), derive the expression for the bias in the estimate of \\(\\beta_1\\) if model (2) is considered.\n(4 points)\n\n\nB.5.2 \nUse the simulated data to compute the value of the bias obtained from the expression derived in the previous question (B.5.1).\n(2 points)\n\n\nB.5.3 \nUse simulations to verify the value of bias computed in the previous question (B.5.2). Simulate the data 1000 times. Set a unique value of seed to generate a unique dataset in every iteration. In each iteration, estimate \\(\\beta_1\\). Report the bias in the estimate of \\(\\beta_1\\) based on the 1000 simulations. Is it the same as obtained analytically in the previous question (B.5.2)?\nNote that: \\(Bias(\\hat{\\beta}_1) = E(\\hat{\\beta}_1 - \\beta_1)\\)\n(5 + 1 = 6 points)\n\n\nB.5.4 \nPlot the 1000 regression lines based on the 1000 coefficient estimates of the no-intercept model (as obtained in B.5.3) over a scatterplot of the data points.\n(2 points)\n\n\nB.5.5 \nNow consider the simple linear regression model with the intercept (model 1). Simulate the data 1000 times. In each iteration, estimate \\(\\beta_1\\). Report the bias in the estimate of \\(\\beta_1\\) based on the 1000 simulations.\n(4 points)\n\n\nB.5.6 \nPlot the 1000 regression lines based on the 1000 coefficient estimates of the model with intercept (as obtained in B.5.5) over a scatterplot of the data points.\n(2 points)\n\n\nB.5.7 \nBased on the simulations and analytical results, answer the following:\n\nB.5.7.1 \nIf the intercept is found to be statistically insignificant in model (1) as shown in the model summary, should it be removed from the model, and model (1) considered instead? Explain.\n(3 points)\n\n\nB.5.7.2 \nSuppose we know that the true model passes through the origin \\((X = 0, Y = 0)\\). Should we consider model (1) or model (2) in this case? Explain.\n(2 points)\n\n\n\nB.5.8 \nIn terms of bias and variance, mention an advantage and a disadvantage of the estimate of \\(\\beta_1\\) from model (2) as compared to that obtained from model (1), if \\(\\beta_0 \\ne 0\\). The plots developed in earlier questions may also help visualize the advantage and disadvantage.\n(2 + 2 = 4 points)"
  },
  {
    "objectID": "Assignment B.html#section-24",
    "href": "Assignment B.html#section-24",
    "title": "Appendix B — Assignment B",
    "section": "B.6 ",
    "text": "B.6 \nProve that the estimate of the intercept \\(\\beta_0\\), i.e., \\(\\hat{\\beta}_0\\) for model (1) has minimum variance among all unbiased linear estimators.\n(5 points)"
  },
  {
    "objectID": "Assignment B.html#section-25",
    "href": "Assignment B.html#section-25",
    "title": "Appendix B — Assignment B",
    "section": "B.7 ",
    "text": "B.7 \nDerive the variance of the estimators obtained in questions A.8 and A.9 of Assignment A.\n(2 + 2 = 4 points)"
  },
  {
    "objectID": "Assignment B.html#section-26",
    "href": "Assignment B.html#section-26",
    "title": "Appendix B — Assignment B",
    "section": "B.8 ",
    "text": "B.8 \nSuppose the true value of \\(\\beta_0\\) is known, while that of \\(\\beta_1\\) is unknown and needs to be estimated. The stakeholder wishes to choose the model (from model (1) and model(2)), such that the expected squared error in the estimate of \\(\\beta_1\\), is the minimum, i.e., they wish to select the model that has a lesser value of \\(E(\\hat{\\beta}_1 - \\beta_1)^2\\). Derive the condition (based on the value \\(\\beta_0\\)), which if true will imply that model (2) should be chosen instead of model (1).\nHint:\n\\(E(X^2) = [E(X)]^2 + Var(X)\\)\n\\(\\implies E(\\hat{\\beta}_1 - \\beta_1)^2 = [E(\\hat{\\beta}_1 - \\beta_1)]^2 + Var(\\hat{\\beta}_1 - \\beta_1)\\)\nModel (2) should be selected when \\(E(\\hat{\\beta}_1 - \\beta_1)^2\\) for model (2) is lesser than \\(E(\\hat{\\beta}_1 - \\beta_1)^2\\) for model (1).\n(7 points)"
  },
  {
    "objectID": "Assignment C.html#instructions",
    "href": "Assignment C.html#instructions",
    "title": "Appendix C — Assignment C",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nMake R code chunks to insert code and type your answer outside the code chunks. Ensure that the solution is written neatly enough to understand and grade.\nRender the file as HTML to submit. For theoretical questions, you can either type the answer and include the solutions in this file, or write the solution on paper, scan and submit separately.\nThe assignment is worth 100 points, and is due on 22nd October 2023 at 11:59 pm.\nThere is an extra credit question worth 10 points in the end. You can score 110/100 in the assignment.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (the theory part may be scanned and submitted separately) (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.). There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\nFinal answers of each question are written clearly (1 pt).\nThe proofs are legible, and clearly written with reasoning provided for every step. They are easy to follow and understand (1 pt)"
  },
  {
    "objectID": "Assignment C.html#real-estate-sales",
    "href": "Assignment C.html#real-estate-sales",
    "title": "Appendix C — Assignment C",
    "section": "C.1 Real estate sales",
    "text": "C.1 Real estate sales\nRead the file real_estate_sales.txt.\n\nC.1.1 \nDevelop a linear regression model to predict price of a house based on square_feet (floor area of the house). Check the following model assumptions using diagnostic plots:\n\nLinear relationship\nHomoscedasticity\nNormal distribution of errors\n\nFor each of the above assumption, comment if it is appears to be satisfied or violated based on the plots.\n(2 + 2 + 2 = 6 points)\n\n\nC.1.2 \nConsider performing the statistical test to check the linear relationship assumption. What condition must be satisfied by the data for the test to be performed? Show that the condition is satisfied by this data.\n(1 + 2 = 3 points)\n\n\nC.1.3 \nPerform the statistical test to check the linear relationship assumption. What is your conclusion?\n(2 points)\n\n\nC.1.4 \nCheck assumptions (2) and (3) as mentioned in C.1.1 with statistical tests, and mention your conclusion.\n(2 + 2 = 4 points)\n\n\nC.1.5 \nUse the Box-Cox procedure to identify the appropriate transformation of the regression model developed in C.1.1. Write the transformed model equation.\n(3 + 2 = 5 points)\n\n\nC.1.6 \nCheck all the 3 model assumptions mentioned in C.1.1 for the transformed model developed in C.1.5. Use both diagnostic plots and statistical tests. Mention your comments based on the plots and conclusions based on the tests.\n(2 + 2 + 2 = 6 points)\n\n\nC.1.7 \nIs the transformed model (developed in C.1.5) better than the original model (developed in C.1.1) with respect to the model assumptions? Comment based on the tests/plots in the previous question (C.1.6).\n(2 points)\n\n\nC.1.8 \nIf the linearity assumption is still not satisfied in the transformed model (developed in C.1.5),\n\nPropose another transformation based on the diagnostic plot(s) plotted in the previous question.\nWrite the transformed model equation.\nShow that the transformed model (based on the proposed transformation in (a)) satisfies the linearity assumption based on the statistical test, if the probability of type I error considered is 1%.\nAlso make the diagnostic plot for the transformed model (based on the proposed transformation in (a)) to show that it satisfies the linearity assumption.\n\n(2 + 2 + 2 + 2 = 8 points)\n\n\nC.1.9 \nDoes the transformed model developed in the previous question (C.1.8) satisfy the homoscedasticity and normally distributed errors assumptions? Verify based on diagnostic plots and statistical tests.\n(2 + 2 = 4 points)\n\n\nC.1.10 \nPlot all the three models - the original model (developed in C.1.1), the boxcox transformed model (developed in C.1.5), and the final model (developed in C.1.8) over a scatterplot of price vs square_feet. Which model seems to have the best fit? Also report the \\(R^2\\) of all the 3 models.\n(1 + 3 + 1 + 3 = 8 points)"
  },
  {
    "objectID": "Assignment C.html#mortality-vs-income",
    "href": "Assignment C.html#mortality-vs-income",
    "title": "Appendix C — Assignment C",
    "section": "C.2 Mortality vs Income",
    "text": "C.2 Mortality vs Income\nThe dataset infmort.csv gives the infant mortality of different countries in the world. The column mortality contains the infant mortality in deaths per 1000 births.\n\nC.2.1 \nRead the dataset. There are 4 observations that have missing values. Remove those observations from the dataset.\nHint: You may use the R function complete.cases().\n(2 points)\n\n\nC.2.2 \nOver the scatterplot of mortality against income, plot the regression model predicting mortality based on income. Report the \\(R^2\\) for this model.\n(2 + 1 = 3 points)\n\n\nC.2.3 \nAre there outlying observations in the data with respect to the model developed in the previous question (C.2.2)? How many? Consider observations having a magnitude of standardized residual more than 5 as outliers.\n(2 points)\n\n\nC.2.4 \nBased on the plot in C.2.2, propose a transformation for income. Justify the proposed transformation. Write the equation of the transformed model and report its \\(R^2\\).\n(2 + 2 + 1 + 1 = 6 points)\n\n\nC.2.5 \nPlot the transformed model developed in the previous question over a scatterplot of mortality vs the transformed income (as transformed in the previous question (C.2.4)). Did the model fit of the transformed model (developed in C.2.4) improve over the fit of the original model (developed in C.2.2)?\n(2 + 1 = 3 points)\n\n\nC.2.6 \nUse Box-Cox to identify the appropriate transformation for mortality in the transformed model (developed in C.2.4). Write the equation of the Box-Cox transformed model, and report its \\(R^2\\).\n(3 points)\n\n\nC.2.7 \nPlot the Box-Cox transformed model developed in the previous question (C.2.6) over a scatterplot of the transformed mortality vs the transformed income. Did the fit of the Box-Cox transformed model improve over the fit of the transformed model (developed in C.2.4)?\n(2 + 1 = 3 points)\n\n\nC.2.8 \nPlot the Box-Cox transformed model (developed in C.2.6) over the scatterplot of mortality vs income.\n(3 points)\n\n\nC.2.9 \nAre there outlying observations in the data with respect to the Box-Cox transformed model (developed in C.2.6)? If not, then how did they disappear given that there were outliers in the original model (developed in C.2.2)?\n(1 + 3 = 4 points)"
  },
  {
    "objectID": "Assignment C.html#section-19",
    "href": "Assignment C.html#section-19",
    "title": "Appendix C — Assignment C",
    "section": "C.3 ",
    "text": "C.3 \nSuppose the error terms in the following linear regression model are independent \\(N(0, \\sigma^2)\\):\n\\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, \\epsilon_i \\sim N(0, \\sigma^2)\\).\n\nC.3.1 \nIf \\(X_i\\) is transformed to \\(X_i'=1/X_i\\), then will the error terms still be independent \\(N(0, \\sigma^2)\\)? If not, then what will be the change in their distribution?\n(4 points)\n\n\nC.3.2 \nInstead of \\(X_i\\), if \\(Y_i\\) is transformed to \\(Y_i'=1/Y_i\\), then will the error terms still be independent \\(N(0, \\sigma^2)\\)? If not, then what will be the change in their distribution?\n(4 points)"
  },
  {
    "objectID": "Assignment C.html#section-22",
    "href": "Assignment C.html#section-22",
    "title": "Appendix C — Assignment C",
    "section": "C.4 ",
    "text": "C.4 \nA simple linear regression model with intercept \\(\\beta_0 = 0\\) is under consideration. Data have been obtained that contain replications.\nState the full and reduced models for testing the appropriateness of the regression function under consideration.\nWhat are the degrees of freedom associated with the full and reduced model if number of observations \\(n=20\\) and number of distinct values of the predictor \\(c=10\\)?\n(2 + 2 = 4 points)"
  },
  {
    "objectID": "Assignment C.html#section-23",
    "href": "Assignment C.html#section-23",
    "title": "Appendix C — Assignment C",
    "section": "C.5 ",
    "text": "C.5 \nLet the observed value of the response variable for the \\(i^{th}\\) replicate of the \\(j^{th}\\) level of the predictor \\(X\\) be \\(Y_{ij}\\), where \\(i=1,...,n_j\\), \\(j= 1,...,c\\).\nThe fitted regression model is:\n\\(\\hat{Y}_{ij} = \\hat{\\beta}_0+\\hat{\\beta}_1X_j\\)\nThe error deviation can be decomposed as the pure error deviation, and the lack of fit deviation as follows:\n\\(Y_{ij}-\\hat{Y}_{ij} = (Y_{ij}-\\bar{Y}_j) + (\\bar{Y}_j-\\hat{Y}_{ij})\\)\nGiven the above equation, show that:\n\\(\\sum_{i=1}^{n_j}\\sum_{j=1}^c(Y_{ij}-\\hat{Y}_{ij})^2 = \\sum_{i=1}^{n_j}\\sum_{j=1}^c(Y_{ij}-\\bar{Y}_j)^2 + \\sum_{i=1}^{n_j}\\sum_{j=1}^c(\\bar{Y}_j-\\hat{Y}_{ij})^2\\).\n(6 points)"
  },
  {
    "objectID": "Assignment C.html#bonus-question",
    "href": "Assignment C.html#bonus-question",
    "title": "Appendix C — Assignment C",
    "section": "C.6 Bonus question",
    "text": "C.6 Bonus question\n(This is extra credit question, you are not required to do it)\nFor \\(\\lambda \\ne 0\\), the Box-Cox transformation is given as:\n\\(y_i^{(\\lambda)}=\\frac{y_i^\\lambda-1}{\\lambda}...(1)\\)\nHowever, typically practitioners transform the response as:\n\\(y_i^{(\\lambda)}=y_i^\\lambda...(2)\\).\nWhy is (2) acceptable in general even when (1) is the transformation proposed by George Box and David Cox? In which cases will it be detrimental to use (2) instead of (1), and one must use (1)?\nSupport your arguments with examples based on simulated data to answer this question. You must provide an example when both (1) and (2) are acceptable, and another example when (2) is not effective, and only (1) must be used.\n(10 points)"
  },
  {
    "objectID": "Assignment D.html#instructions",
    "href": "Assignment D.html#instructions",
    "title": "Appendix D — Assignment D",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nMake R code chunks to insert code and type your answer outside the code chunks. Ensure that the solution is written neatly enough to understand and grade.\nRender the file as HTML to submit. For theoretical questions, you can either type the answer and include the solutions in this file, or write the solution on paper, scan and submit separately.\nThe assignment is worth 100 points, and is due on 5th November 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (the theory part may be scanned and submitted separately) (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.). There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\nFinal answers of each question are written clearly (1 pt).\nThe proofs are legible, and clearly written with reasoning provided for every step. They are easy to follow and understand (1 pt)"
  },
  {
    "objectID": "Assignment D.html#ols-estimator-bias",
    "href": "Assignment D.html#ols-estimator-bias",
    "title": "Appendix D — Assignment D",
    "section": "D.1 OLS estimator bias",
    "text": "D.1 OLS estimator bias\nShow that the least squares estimator \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) is unbiased.\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#variance-covariace-of-fitted-values",
    "href": "Assignment D.html#variance-covariace-of-fitted-values",
    "title": "Appendix D — Assignment D",
    "section": "D.2 Variance-covariace of fitted values",
    "text": "D.2 Variance-covariace of fitted values\nObtain the expression for the variance-covariance matrix of the fitted values \\(\\hat{Y}_i, i = 1,...,n\\) in terms of the hat matrix \\(H\\).\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#maximum-likelihood-estimation-for-generalized-least-squares",
    "href": "Assignment D.html#maximum-likelihood-estimation-for-generalized-least-squares",
    "title": "Appendix D — Assignment D",
    "section": "D.3 Maximum likelihood estimation for Generalized Least Squares",
    "text": "D.3 Maximum likelihood estimation for Generalized Least Squares\nThe density of the multivariate normal distribution is:\n\\(f(Y) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\bigg[-\\frac{1}{2}(Y-\\mu)^T\\Sigma^{-1}(Y-\\mu)\\bigg]\\)\nFor the linear regression model:\n\\(Y = X\\beta+\\epsilon, \\epsilon \\sim N(0, \\Sigma)\\),\nderive the maximum likelihood estimate of the regression coefficient vector \\(\\beta\\).\nWhat is the Hat matrix?\nNote: The variance-covariance matrix of the error term is \\(\\Sigma\\), which implies that the error terms may or may not be correlated, and the variance of the error terms may not be constant. You are deriving the estimates for this general scenario.\n(6 + 2 = 8 points)"
  },
  {
    "objectID": "Assignment D.html#general-linear-regression-model",
    "href": "Assignment D.html#general-linear-regression-model",
    "title": "Appendix D — Assignment D",
    "section": "D.4 General Linear Regression model",
    "text": "D.4 General Linear Regression model\nFor each of the following regression models, indicate if it can be transformed to a general linear regression model. Assume \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Justify your answer, i.e., mention the appropriate transformation(s). Note that the assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) need to hold for a general linear regression model.\n\n\\(Y_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2\\log(X_{i2})+\\beta_3X_{i1}^2+\\epsilon_i\\)\n\\(Y_i = \\epsilon_i\\exp(\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}^2)\\)\n\\(Y_i = \\log(\\beta_1X_{i1})+\\beta_2X_{i2}+\\epsilon_i\\)\n\\(Y_i = \\beta_0\\exp(\\beta_1X_{i1})+\\epsilon_i\\)\n\\(Y_i = [1 + \\exp(\\beta_0+\\beta_1X_{i1}+\\epsilon_i)]^{-1}\\)\n\n(5x2 = 10 points)"
  },
  {
    "objectID": "Assignment D.html#cory_i-haty_i-sqrtr2",
    "href": "Assignment D.html#cory_i-haty_i-sqrtr2",
    "title": "Appendix D — Assignment D",
    "section": "D.5 \\(Cor(Y_i, \\hat{Y}_i) = \\sqrt(R^2)\\)",
    "text": "D.5 \\(Cor(Y_i, \\hat{Y}_i) = \\sqrt(R^2)\\)\nFor the multiple linear regression model, show that the square of the correlation between the response \\(Y_i\\) and the fitted values \\(\\hat{Y}_i\\) is the coefficient of determination \\(R^2\\)\nHint:\nFor random variables \\(a\\) and \\(b\\):\n\\(Cov(a, a) = Var(a)\\),\n\\([Cor(a, b)]^2 = \\frac{[Cov(a,b)]^2}{Var(a)Var(b)}\\).\nFor the linear regression model:\n\\(Y = \\hat{Y}+\\epsilon\\)\n(8 points)"
  },
  {
    "objectID": "Assignment D.html#developing-the-mlr-model",
    "href": "Assignment D.html#developing-the-mlr-model",
    "title": "Appendix D — Assignment D",
    "section": "D.6 Developing the MLR model",
    "text": "D.6 Developing the MLR model\nRead the dataset house_prices.csv.\n\nD.6.1 \nMake a pairplot. Which predictors seem to be useful to predict house_price? Ignore house_id while making the pairplot.\n(2+2 = 4 points)\n\n\nD.6.2 \nPrint the pairwise correlation matrix. Which predictors seem to be useful to predict house_price? Ignore house_id while printing the correlation matrix.\n(2+2 = 4 points)\n\n\nD.6.3 \nDevelop a linear regression model to predict house_price based on house_age, distance_MRT, number_convenience_stores, and latitude. Report the model \\(R^2\\) and \\(R^2_{adj}\\)\n(2+2 = 4 points)\n\n\nD.6.4 \nMake the diagnostic plots to verify if the model satisfies the assumptions of:\n\nLinear relationship\nHomoscedasticity\nNormal distribution of errors\n\nAlso verify the assumptions of homoscedasticity and normal distribution of errors with statistical tests.\n(2x3 + 2 = 8 points)\n\n\nD.6.5 \nPlot the residuals against each predictor. For each predictor, comment if it seems to have a linear relationship with the response, and if the error variance seems constant.\n(4x2 = 8 points)\n\n\nD.6.6 \nGiven the analysis in the previous plots (in D.6.4 and D.6.5), will it be appropriate to transform the predictors or the response or both? Why? If both, then which should you transform first?\n(1+2+1 = 4 points)\n\n\nD.6.7 \nUse the Box-Cox procedure to transform the model. Is the Box-Cox model an improvement over the previous model with regard to goodness-of-fit?\n(2+2 = 4 points)\n\n\nD.6.8 \nMake diagnostic plots to verify if the Box-Cox model developed in the previous question satisfies the 3 assumptions mentioned in D.6.4. Is the Box-Cox model an improvement over the previous model with regard to the assumptions? Explain.\n(2+2 = 4 points)\n\n\nD.6.9 \nPlot the residuals against each predictor for the Box-Cox model developed in D.6.7. For each predictor, comment if it seems to have a linear relationship with the response, and if the error variance seems constant.\n(4x2 = 8 points)\n\n\nD.6.10 \nBased on the above plots, transform two predictors of your choice to improve the model on the linear relationship assumption with regard to the predictors. Mention the intuition behind the transformations.\n(2 + 2 = 4 points)\n\n\nD.6.11 \nMake the diagnostic plots, and comment if the model developed in D.6.10 has further improved over the Box-Cox model developed in D.6.7 with regard to the 3 model assumptions and goodness-of-fit, and if it seems to satisfy the 3 assumptions.\n(3x2 + 2 + 3 = 11 points)"
  },
  {
    "objectID": "Assignment E.html#instructions",
    "href": "Assignment E.html#instructions",
    "title": "Appendix E — Assignment E",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nMake R code chunks to insert code and type your answer outside the code chunks. Ensure that the solution is written neatly enough to understand and grade.\nRender the file as HTML to submit. For theoretical questions, you can either type the answer and include the solutions in this file, or write the solution on paper, scan and submit separately.\nThe assignment is worth 100 points, and is due on 12th November 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (the theory part may be scanned and submitted separately) (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.). There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\nFinal answers of each question are written clearly (1 pt).\nThe proofs are legible, and clearly written with reasoning provided for every step. They are easy to follow and understand (1 pt)"
  },
  {
    "objectID": "Assignment E.html#prostate",
    "href": "Assignment E.html#prostate",
    "title": "Appendix E — Assignment E",
    "section": "E.1 Prostate",
    "text": "E.1 Prostate\nA study was conducted on 97 men with prostate cancer who were due to receive a radical prostatectomy. The dataset prostate.csv contains data on 9 measurements made on these 97 men. The description of variables can be found here.\n\nE.1.1 Fitting the model\nFit a linear regression model with lpsa as the response and the other variables as the predictors. Write down the equation to predict lpsa based on the other eight variables. When coding the model in R, do not type the names of all the predictors.\n(2 points)\n\n\nE.1.2 Model significance\nIs the overall regression significant at 5% level? Justify your answer.\n(2 points)\n\n\nE.1.3 Interpretation\nInterpret the coefficient of svi.\n(3 points)\n\n\nE.1.4 Effect of other predictors\nFit a simple linear regression on lpsa against gleason. Is the predictor gleason statistically significant in this model?\nWas gleason statistically significant in the model developed in the first question (E.1.1) with multiple predictors?\nDid the statistical significance of gleason change in the absence of other predictors? Why or why not?\n(1 + 1 + 1 + 2 = 5 points)\n\n\nE.1.5 Relevant subset of predictors\nFind the largest subset of predictors in the model developed in the first question (E.1.1), such that their coefficients are zero, i.e., none of the predictors in the subset are statistically significant. Assume a significance level of 5%.\nDoes the model \\(R^2_{adj}\\) change a lot if you remove the set of predictors identified above from the model in the first question? Why or why not?\n(4 + 2 = 6 points)"
  },
  {
    "objectID": "Assignment E.html#infant-mortality",
    "href": "Assignment E.html#infant-mortality",
    "title": "Appendix E — Assignment E",
    "section": "E.2 Infant mortality",
    "text": "E.2 Infant mortality\nThe dataset infmort.csv gives the infant mortality of different countries in the world. The column mortality contains the infant mortality in deaths per 1000 births.\n\nE.2.1 Data visualisation\nContinue with the model developed in C.2.6. Visualize the distribution of the transformed response in C.2.6 for each region, i.e., each continent. Based on the plots, does region seem to be associated with infant mortality.\nNote: You may use the R function boxplot() to visualize the distribution.\n(2 + 1 = 3 points)\n\n\nE.2.2 MLR\nUpdate the model developed in in C.2.6 by adding region as a predictor. Use this model to compute adjusted_mortality for each observation in the data, where adjusted mortality is the mortality after removing the estimated effect of income. Make a boxplot of log(adjusted_mortality) against region.\n(2 + 1 = 3 points)\n\n\nE.2.3 Analysis\nFrom the plot in the previous question (E.2.2):\n\nDoes Europe still seem to have the lowest mortality as compared to other regions after removing the effect of income from mortality?\nAfter adjusting for income, is there any change in the mortality comparison among different regions. Compare the plot developed in the previous question (E.2.2) to the plot of log(mortality) against region developed earlier (E.2.1) to answer this question.\n\nHint: Do any African / Asian / American countries seem to do better than all the European countries with regard to mortality after adjusting for income?\n(1 + 3 = 4 points)"
  },
  {
    "objectID": "Assignment E.html#gdp-per-capita",
    "href": "Assignment E.html#gdp-per-capita",
    "title": "Appendix E — Assignment E",
    "section": "E.3 GDP per capita",
    "text": "E.3 GDP per capita\nThe dataset soc_ind.csv contains the GDP per capita of some countries along with several social indicators.\n\nE.3.1 Best predictor\nFor a simple linear regression model predicting gdpPerCapita, justify that lifeFemale (female life expectancy) will provide the best model fit (ignore categorical predictors)?\n(1 point)\n\n\nE.3.2 Polynomial transformation\nDevelop a linear regression model to predict gdpPerCapita based on the appropriate polynomial transformation of lifeFemale. Find the appropriate polynomial transformation using the General Linear test approach. Support your choice of the appropriate polynomial transformation with a plot of the model on the data, showing the model fit.\n(3 + 2 = 5 points)\n\n\nE.3.3 VIF\nIs there multicollinearity in the model? Find the VIF and comment.\n(2 points)\n\n\nE.3.4 Centering\nCenter the predictor lifeFemale and re-develop the model. Did the severity of multicollinearity reduce in the updated model? If yes, what is the benefit of the reduced multicollinearity. Use the model summary of both the models as an evidence to support your arguments.\n(2 + 2 + 2 + 2 = 8 points)\n\n\nE.3.5 Interpretation\nDevelop a model to predict gdpPerCapita with lifeFemale and continent as predictors.\n\nInterpret the intercept term.\nFor a given value of lifeFemale, are there any continents that do not have a significant difference between their mean gdpPerCapita and that of Africa? If yes, then which ones, and why? If no, then why not? Consider a significance level of 5%.\n\n(2 + 2 = 4 points)\n\n\nE.3.6 Interaction\nThe model developed in the previous question has a limitation. It assumes that the increase in mean gdpPerCapita with a unit increase in lifeFemale does not depend on the continent.\n\nEliminate this limitation by including interaction of continent with lifeFemale in the model developed in the previous question (E.3.5). Print the model summary of the model with interactions.\nInterpret the coefficient of any one of the interaction terms.\n\n(1 + 2 = 3 points)\nNote: Use the model developed in this question (E.3.6) for all the questions below.\n\n\nE.3.7 Model visualisation\nUse the model developed in (E.3.6) to plot regression lines for Africa, Asia, and Europe. Put gdpPerCapita on the vertical axis and lifeFemale on the horizontal axis. Use a legend to distinguish among the regression lines of the three continents.\n(3 points)\n\n\nE.3.8 Analysing the model\nBased on the plot develop in the previous question (E.3.7), which continent has the highest increase in mean gdpPerCapita for a unit increase in lifeFemale, and which one has the least? Justify your answer.\n(1 + 1 + 2 = 4 points)\n\n\nE.3.9 Quantifying difference in slope\nOn an average, how much higher is the increase in GDP per Capita of a European country than increase in GDP per Capita of an Asian country for an increase of 1 year in female life expectancy?\n(2 points)\n\n\nE.3.10 Quantifying uncertainty of difference in slope\nFind the 95% confidence interval of the metric obtained in the previous question.\nIn other words, find the 95% confidence interval of the difference of increase in GDP per Capita of a European country and increase in GDP per Capita of an Asian country, for an increase of 1 year in female life expectancy.\nNote: You may use the R function vcov() to find the variance-covariance matrix of the regression coefficients:\n\\(Var(\\hat{\\beta}) = \\sigma^2(X^TX)^{-1}\\)\n(5 points)\n\n\nE.3.11 Quantifying difference in prediction\nOn an average, for a female life expectancy of 72 years, how much higher is the GDP per Capita of an Asian country than the GDP per Capita of a European country?\n(5 points)\n\n\nE.3.12 Uncertainty of difference in prediction\nFind the 95% confidence interval of the metric obtained in the previous question.\nIn other words, find the 95% confidence interval of the difference of GDP per Capita of an Asian country and the GDP per Capita of a European country, for a female life expectancy of 72 years.\n(10 points)\nNote: You may use the R function vcov() to find the variance-covariance matrix of the regression coefficients:\n\\(Var(\\hat{\\beta}) = \\sigma^2(X^TX)^{-1}\\)\n\n\nE.3.13 Statistical significance\nBased on the confidence interval obtained in the question (E.3.12), is the GDP per Capita of an Asian country significantly different from the GDP per Capita of a European country, for a female life expectancy of 72 years?\n(1 point)\n\n\nE.3.14 Modeling approach\nThe developed model (in E.3.6) can be used to predict the GDP per Capita for any country, based on its female life expectancy and continent.\nInstead of developing a single model, consider the scenario that a distinct model is developed for each country, i.e., a distinct model is developed for Asia based on the data only corresponding to Asia, a distinct model is developed for Europe based on the data only corresponding to Europe, and so on. Now answer the following questions with justification for each:\n\nE.3.14.1 \nWill the expected GDP per capita of a country for a given female life expectancy from the model developed in E.3.6 be different from the model developed only based on the country’s continent data?\n\n\nE.3.14.2 \nWill the confidence interval of the expected GDP per capita of a country for a given female life expectancy from the model developed in E.3.6 be different from the model developed only based on the country’s continent data?\n\n\nE.3.14.3 \nWhat is the advantage of one approach over the other, and vice-versa? In other words, what is the advantage of the approach of developing a single model for all continents (as in E.3.6) in contrast to developing a separate model for each continent. And, what is the advantage of the approach of developing a separate model for each continent in contrast to developing a single model for all continents.\n(4 + 4 + 6 = 14 points)"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix F — Assignment templates and Datasets",
    "section": "",
    "text": "Assignment templates and datasets used in the class notes can be found here"
  }
]