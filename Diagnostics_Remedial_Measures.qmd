# Diagnostics and Remedial measures

Let us consider the `Auto` dataset from the `ISLR` library. The library is related to the book *[Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)*

## Loading libraries

```{r}
library(ISLR)
library(ggplot2)
library(lmtest)
```


## Reading data

```{r}
auto_data <- Auto
```

## Developing model

Suppose we wish to predict `mpg` based on `horsepower`.

```{r}
linear_model <- lm(mpg~horsepower, data = auto_data)
```

## Model fit

Let us visualize the model fit to the data.

```{r}
ggplot(data = auto_data, aes(x = horsepower, y = mpg))+
  geom_point()+
  geom_smooth(method = "lm")
```

The plot indicates that a quadratic relationship between `mpg` and `horsepower` may be more appropriate. 

## Diagnostic plots

Let us make diagnostic plots that help us check the model assumptions.

```{r}
par(mfrow = c(2,2))
plot(linear_model)
```

### Linear relationship

#### Visual check

We can check the linearity assumption visually by analyzing the plot of residuals against fitted values. The plot indicates that a quadratic relationship between the response and the predictor may be more appropriate than a linear one.

#### Statistical test

We'll use the *F* test for lack of fit to check the linearity assumption

```{r}
#Full model
full_model <- lm(mpg~as.factor(horsepower), data = auto_data)
```

```{r}
# F test for lack of fit
anova(linear_model, full_model)
```

As expected based on the visual check, the linear model fails the linearity test with a very low $p$-value of the order of $10^{-14}$. Thus, we conclude that relationship is non-linear.

### Constant variance (Homoscedasticity)

#### Visual check

The plot of the residuals against fitted values indicates that the error variance is increasing with increasing values of predicted `mpg` or decreasing `horsepower`. Thus, there seems to be heteroscedasticity.

#### Statistical test

**Breusch-Pagan test**

Let us conduct the Breusch-Pagan test for homoscedasticity. This is a large sample test, which assumes that the error terms are independent and normally distruted, and that the variance of of the error term $\epsilon_i$, denoted by $\sigma_i^2$, is related to the level of the predictor $X$ in the following way:

$\log(\sigma_i^2) = \gamma_0 + \gamma_1X_i$.

This implies that $\sigma_i^2$ either increases or decreases with the level of $X$. Constant error variance corresponds to $\gamma_1 = 0$. The hypotheses for the test are:

$H_0: \gamma_1 = 0$
$H_a: \gamma_1 \ne 0$

```{r}
bptest(linear_model)
```

As expected based on the visual check, the model fails the homoscedasticity with a low $p$-value of 0.3\%.


**Brown-Forsythe test**

To conduct the Brown-Forsythe test, we divide the data into two groups, based on the predictor $X$. If the error variance is not constant, the residuals in one group will tend to be more variable than those in the other group. The test consists of a two-sample $t$-test to determine whether the mean of the absolute deviations from the median residual of one group differs significantly from the mean of the absolute deviations from the median residual of the other group.

```{r}
#Break the residuals into 2 groups
residuals_group1 <- linear_model$residuals[auto_data$horsepower<=100]
residuals_group2 <- linear_model$residuals[auto_data$horsepower>100]
```

```{r}
#Obtain the median of each group
median_group1 <- median(residuals_group1)
median_group2 <- median(residuals_group2)
```

```{r}
#Two-sample t-test
t.test(abs(residuals_group1-median_group1), abs(residuals_group2-median_group2), var.equal = TRUE)
```

Brown-Forsythe test also shows that there is heteroscedasticity.

### Normality of error terms

#### Visual check

The QQ plot indicates that the distribution of the residuals is slightly right-skewed. This right-skew can be visualized by making a histogram or a density plot of residuals:

```{r}
par(mfrow = c(1,1))
hist(linear_model$residuals, breaks = 20, prob = TRUE)
lines(density(linear_model$residuals), col = 'blue')
```

#### Statistical test

```{r}
shapiro.test(linear_model$residuals)
```

As expected based on the visual check, the model fails the test for normal distribution of error terms with a low $p$-value of the order of $10^{-5}$.

### Independence of error terms

This test is relevant if there is an order in the data, i.e., if the observations can be arranged based on time, space, etc. In the current example, there is no order, and so this test is not relevant.

## Remedial Measures

If the linear relationship assumption violated, it results in biased estimates of the regression coefficients resulting in a biased prediction. As the estimates are biased, their variance estimates may also be inaccurate. Thus, this is a very important assumption as it leads to inaccuracies in both point estimates and statistical inference. 

If the homoscedasticity assumption is violated, but the linear relationship assumption holds, the OLS estimates are still unbiased resulting in an unbiased prediction. However, their standard error estimates are likely to be inaccurate. 

If the assumption regarding the normal distribution of errors is violated, the OLS estimates are still BLUE (Best linear unbiased estimates). The confidence interval for a point estimate, though effected, is robust to departures from normality for large sample sizes. However, the prediction interval is sensitive to the same.

Depending on the application, one assumption may be more important for the stakeholder than the other. 


